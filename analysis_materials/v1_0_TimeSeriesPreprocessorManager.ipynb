{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "141dff1e-7d1d-4817-961d-f1c370f7c943",
   "metadata": {},
   "source": [
    "###  First unoptimized version packed into one class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bc531e5-9e6a-4c46-80e5-ad1b29104497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37690d97-d9bc-4739-bb86-1776d8203579",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "# для TimeSeriesPreprocessorManager\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import statsmodels.api as sm\n",
    "from prophet import Prophet\n",
    "from statsmodels.tsa.seasonal import STL, seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf, kpss\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from scipy.stats import zscore\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import pmdarima as pm  # INSTALL TO REQ\n",
    "from scipy import signal\n",
    "from scipy.stats import zscore, probplot, pearsonr\n",
    "from joblib import Parallel, delayed\n",
    "from tbats import TBATS\n",
    "import holidays\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from tbats import TBATS\n",
    "from prophet import Prophet\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.early_stop import no_progress_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe73e061-4bf4-4a5d-bb26-6a2cb383f25c",
   "metadata": {},
   "source": [
    "# Receiving input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "729332dc-1015-4167-9c03-1c03dd43284a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLICKHOUSE_HOST='localhost'\n",
    "CLICKHOUSE_PORT=8123\n",
    "CLICKHOUSE_USER='CLICKHOUSE_USER' # INPUT YOUR CREDENTIAL \n",
    "CLICKHOUSE_PASSWORD='CLICKHOUSE_PASSWORD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9a8e13e-9f01-441d-a457-06181c929eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain data\n",
    "import clickhouse_connect\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "client = clickhouse_connect.get_client(\n",
    "                host=CLICKHOUSE_HOST,\n",
    "                port=CLICKHOUSE_PORT,\n",
    "                user=CLICKHOUSE_USER,\n",
    "                password=CLICKHOUSE_PASSWORD\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8836d05-55f0-459e-93ff-725064b8d4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load time series from db\n",
    "tablename = \"CRYPTO.btcusdt_12h_raw\"\n",
    "ch_response = client.query(f\"SELECT * FROM {tablename} ORDER BY Open_time\")\n",
    "ts_data = pd.DataFrame(ch_response.result_rows, columns=ch_response.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "921f54ab-370d-4593-830e-c8fc7f197936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open_time</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Close_time</th>\n",
       "      <th>Quote_asset_volume</th>\n",
       "      <th>Number_of_trades</th>\n",
       "      <th>Taker_buy_base_asset_volume</th>\n",
       "      <th>Taker_buy_quote_asset_volume</th>\n",
       "      <th>ts_id</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-08-17 00:00:00</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4485.39</td>\n",
       "      <td>4261.32</td>\n",
       "      <td>4427.30</td>\n",
       "      <td>145.708747</td>\n",
       "      <td>2017-08-17 11:59:59</td>\n",
       "      <td>6.356955e+05</td>\n",
       "      <td>582</td>\n",
       "      <td>122.801360</td>\n",
       "      <td>5.367015e+05</td>\n",
       "      <td>btcusdt_12h</td>\n",
       "      <td>binance_api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-08-17 12:00:00</td>\n",
       "      <td>4436.06</td>\n",
       "      <td>4485.39</td>\n",
       "      <td>4200.74</td>\n",
       "      <td>4285.08</td>\n",
       "      <td>649.441630</td>\n",
       "      <td>2017-08-17 23:59:59</td>\n",
       "      <td>2.819075e+06</td>\n",
       "      <td>2845</td>\n",
       "      <td>493.447181</td>\n",
       "      <td>2.141515e+06</td>\n",
       "      <td>btcusdt_12h</td>\n",
       "      <td>binance_api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-08-18 00:00:00</td>\n",
       "      <td>4285.08</td>\n",
       "      <td>4371.52</td>\n",
       "      <td>4134.61</td>\n",
       "      <td>4340.31</td>\n",
       "      <td>720.722201</td>\n",
       "      <td>2017-08-18 11:59:59</td>\n",
       "      <td>3.085356e+06</td>\n",
       "      <td>3051</td>\n",
       "      <td>585.408001</td>\n",
       "      <td>2.508148e+06</td>\n",
       "      <td>btcusdt_12h</td>\n",
       "      <td>binance_api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-08-18 12:00:00</td>\n",
       "      <td>4320.52</td>\n",
       "      <td>4340.31</td>\n",
       "      <td>3938.77</td>\n",
       "      <td>4108.37</td>\n",
       "      <td>479.166063</td>\n",
       "      <td>2017-08-18 23:59:59</td>\n",
       "      <td>2.001602e+06</td>\n",
       "      <td>2182</td>\n",
       "      <td>387.460709</td>\n",
       "      <td>1.620975e+06</td>\n",
       "      <td>btcusdt_12h</td>\n",
       "      <td>binance_api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-08-19 00:00:00</td>\n",
       "      <td>4108.37</td>\n",
       "      <td>4184.69</td>\n",
       "      <td>3850.00</td>\n",
       "      <td>3957.60</td>\n",
       "      <td>298.518569</td>\n",
       "      <td>2017-08-19 11:59:59</td>\n",
       "      <td>1.214383e+06</td>\n",
       "      <td>1559</td>\n",
       "      <td>213.297785</td>\n",
       "      <td>8.706680e+05</td>\n",
       "      <td>btcusdt_12h</td>\n",
       "      <td>binance_api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017-08-19 12:00:00</td>\n",
       "      <td>3945.12</td>\n",
       "      <td>4149.99</td>\n",
       "      <td>3928.89</td>\n",
       "      <td>4139.98</td>\n",
       "      <td>82.791194</td>\n",
       "      <td>2017-08-19 23:59:59</td>\n",
       "      <td>3.351012e+05</td>\n",
       "      <td>594</td>\n",
       "      <td>61.038257</td>\n",
       "      <td>2.473339e+05</td>\n",
       "      <td>btcusdt_12h</td>\n",
       "      <td>binance_api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2017-08-20 00:00:00</td>\n",
       "      <td>4120.98</td>\n",
       "      <td>4211.08</td>\n",
       "      <td>4032.62</td>\n",
       "      <td>4106.53</td>\n",
       "      <td>71.235136</td>\n",
       "      <td>2017-08-20 11:59:59</td>\n",
       "      <td>2.930715e+05</td>\n",
       "      <td>622</td>\n",
       "      <td>46.165527</td>\n",
       "      <td>1.899877e+05</td>\n",
       "      <td>btcusdt_12h</td>\n",
       "      <td>binance_api</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Open_time     Open     High      Low    Close      Volume  \\\n",
       "0 2017-08-17 00:00:00  4261.48  4485.39  4261.32  4427.30  145.708747   \n",
       "1 2017-08-17 12:00:00  4436.06  4485.39  4200.74  4285.08  649.441630   \n",
       "2 2017-08-18 00:00:00  4285.08  4371.52  4134.61  4340.31  720.722201   \n",
       "3 2017-08-18 12:00:00  4320.52  4340.31  3938.77  4108.37  479.166063   \n",
       "4 2017-08-19 00:00:00  4108.37  4184.69  3850.00  3957.60  298.518569   \n",
       "5 2017-08-19 12:00:00  3945.12  4149.99  3928.89  4139.98   82.791194   \n",
       "6 2017-08-20 00:00:00  4120.98  4211.08  4032.62  4106.53   71.235136   \n",
       "\n",
       "           Close_time  Quote_asset_volume  Number_of_trades  \\\n",
       "0 2017-08-17 11:59:59        6.356955e+05               582   \n",
       "1 2017-08-17 23:59:59        2.819075e+06              2845   \n",
       "2 2017-08-18 11:59:59        3.085356e+06              3051   \n",
       "3 2017-08-18 23:59:59        2.001602e+06              2182   \n",
       "4 2017-08-19 11:59:59        1.214383e+06              1559   \n",
       "5 2017-08-19 23:59:59        3.351012e+05               594   \n",
       "6 2017-08-20 11:59:59        2.930715e+05               622   \n",
       "\n",
       "   Taker_buy_base_asset_volume  Taker_buy_quote_asset_volume        ts_id  \\\n",
       "0                   122.801360                  5.367015e+05  btcusdt_12h   \n",
       "1                   493.447181                  2.141515e+06  btcusdt_12h   \n",
       "2                   585.408001                  2.508148e+06  btcusdt_12h   \n",
       "3                   387.460709                  1.620975e+06  btcusdt_12h   \n",
       "4                   213.297785                  8.706680e+05  btcusdt_12h   \n",
       "5                    61.038257                  2.473339e+05  btcusdt_12h   \n",
       "6                    46.165527                  1.899877e+05  btcusdt_12h   \n",
       "\n",
       "        Source  \n",
       "0  binance_api  \n",
       "1  binance_api  \n",
       "2  binance_api  \n",
       "3  binance_api  \n",
       "4  binance_api  \n",
       "5  binance_api  \n",
       "6  binance_api  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_data.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc2c461d-3964-4c5c-ae49-d449b6329542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open_time</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Close_time</th>\n",
       "      <th>Quote_asset_volume</th>\n",
       "      <th>Number_of_trades</th>\n",
       "      <th>Taker_buy_base_asset_volume</th>\n",
       "      <th>Taker_buy_quote_asset_volume</th>\n",
       "      <th>ts_id</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6147</th>\n",
       "      <td>2026-01-15 12:00:00</td>\n",
       "      <td>96581.07</td>\n",
       "      <td>97165.03</td>\n",
       "      <td>95134.48</td>\n",
       "      <td>95604.80</td>\n",
       "      <td>14054.49528</td>\n",
       "      <td>2026-01-15 23:59:59</td>\n",
       "      <td>1.352538e+09</td>\n",
       "      <td>2932815</td>\n",
       "      <td>7085.80884</td>\n",
       "      <td>6.825182e+08</td>\n",
       "      <td>btcusdt_12h</td>\n",
       "      <td>binance_api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6148</th>\n",
       "      <td>2026-01-16 00:00:00</td>\n",
       "      <td>95604.81</td>\n",
       "      <td>95871.47</td>\n",
       "      <td>95133.03</td>\n",
       "      <td>95429.09</td>\n",
       "      <td>5116.45507</td>\n",
       "      <td>2026-01-16 11:59:59</td>\n",
       "      <td>4.887122e+08</td>\n",
       "      <td>1009516</td>\n",
       "      <td>2187.83828</td>\n",
       "      <td>2.090544e+08</td>\n",
       "      <td>btcusdt_12h</td>\n",
       "      <td>binance_api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6149</th>\n",
       "      <td>2026-01-16 12:00:00</td>\n",
       "      <td>95429.09</td>\n",
       "      <td>95809.08</td>\n",
       "      <td>94293.46</td>\n",
       "      <td>95550.94</td>\n",
       "      <td>6051.25953</td>\n",
       "      <td>2026-01-16 23:59:59</td>\n",
       "      <td>5.753072e+08</td>\n",
       "      <td>1962043</td>\n",
       "      <td>2867.23124</td>\n",
       "      <td>2.726688e+08</td>\n",
       "      <td>btcusdt_12h</td>\n",
       "      <td>binance_api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6150</th>\n",
       "      <td>2026-01-17 00:00:00</td>\n",
       "      <td>95550.94</td>\n",
       "      <td>95578.20</td>\n",
       "      <td>95021.67</td>\n",
       "      <td>95323.86</td>\n",
       "      <td>1694.35238</td>\n",
       "      <td>2026-01-17 11:59:59</td>\n",
       "      <td>1.614328e+08</td>\n",
       "      <td>459123</td>\n",
       "      <td>839.78958</td>\n",
       "      <td>8.001233e+07</td>\n",
       "      <td>btcusdt_12h</td>\n",
       "      <td>binance_api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6151</th>\n",
       "      <td>2026-01-17 12:00:00</td>\n",
       "      <td>95323.85</td>\n",
       "      <td>95639.45</td>\n",
       "      <td>95050.00</td>\n",
       "      <td>95147.77</td>\n",
       "      <td>2586.44899</td>\n",
       "      <td>2026-01-17 23:59:59</td>\n",
       "      <td>2.467292e+08</td>\n",
       "      <td>561404</td>\n",
       "      <td>1220.06325</td>\n",
       "      <td>1.163867e+08</td>\n",
       "      <td>btcusdt_12h</td>\n",
       "      <td>binance_api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6152</th>\n",
       "      <td>2026-01-18 00:00:00</td>\n",
       "      <td>95147.77</td>\n",
       "      <td>95295.39</td>\n",
       "      <td>94876.33</td>\n",
       "      <td>95168.06</td>\n",
       "      <td>2386.33318</td>\n",
       "      <td>2026-01-18 11:59:59</td>\n",
       "      <td>2.269771e+08</td>\n",
       "      <td>513805</td>\n",
       "      <td>1112.37410</td>\n",
       "      <td>1.058134e+08</td>\n",
       "      <td>btcusdt_12h</td>\n",
       "      <td>binance_api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6153</th>\n",
       "      <td>2026-01-18 12:00:00</td>\n",
       "      <td>95168.06</td>\n",
       "      <td>95376.24</td>\n",
       "      <td>94910.00</td>\n",
       "      <td>95183.55</td>\n",
       "      <td>1256.21349</td>\n",
       "      <td>2026-01-18 23:59:59</td>\n",
       "      <td>1.194810e+08</td>\n",
       "      <td>279502</td>\n",
       "      <td>566.66885</td>\n",
       "      <td>5.389214e+07</td>\n",
       "      <td>btcusdt_12h</td>\n",
       "      <td>binance_api</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Open_time      Open      High       Low     Close       Volume  \\\n",
       "6147 2026-01-15 12:00:00  96581.07  97165.03  95134.48  95604.80  14054.49528   \n",
       "6148 2026-01-16 00:00:00  95604.81  95871.47  95133.03  95429.09   5116.45507   \n",
       "6149 2026-01-16 12:00:00  95429.09  95809.08  94293.46  95550.94   6051.25953   \n",
       "6150 2026-01-17 00:00:00  95550.94  95578.20  95021.67  95323.86   1694.35238   \n",
       "6151 2026-01-17 12:00:00  95323.85  95639.45  95050.00  95147.77   2586.44899   \n",
       "6152 2026-01-18 00:00:00  95147.77  95295.39  94876.33  95168.06   2386.33318   \n",
       "6153 2026-01-18 12:00:00  95168.06  95376.24  94910.00  95183.55   1256.21349   \n",
       "\n",
       "              Close_time  Quote_asset_volume  Number_of_trades  \\\n",
       "6147 2026-01-15 23:59:59        1.352538e+09           2932815   \n",
       "6148 2026-01-16 11:59:59        4.887122e+08           1009516   \n",
       "6149 2026-01-16 23:59:59        5.753072e+08           1962043   \n",
       "6150 2026-01-17 11:59:59        1.614328e+08            459123   \n",
       "6151 2026-01-17 23:59:59        2.467292e+08            561404   \n",
       "6152 2026-01-18 11:59:59        2.269771e+08            513805   \n",
       "6153 2026-01-18 23:59:59        1.194810e+08            279502   \n",
       "\n",
       "      Taker_buy_base_asset_volume  Taker_buy_quote_asset_volume        ts_id  \\\n",
       "6147                   7085.80884                  6.825182e+08  btcusdt_12h   \n",
       "6148                   2187.83828                  2.090544e+08  btcusdt_12h   \n",
       "6149                   2867.23124                  2.726688e+08  btcusdt_12h   \n",
       "6150                    839.78958                  8.001233e+07  btcusdt_12h   \n",
       "6151                   1220.06325                  1.163867e+08  btcusdt_12h   \n",
       "6152                   1112.37410                  1.058134e+08  btcusdt_12h   \n",
       "6153                    566.66885                  5.389214e+07  btcusdt_12h   \n",
       "\n",
       "           Source  \n",
       "6147  binance_api  \n",
       "6148  binance_api  \n",
       "6149  binance_api  \n",
       "6150  binance_api  \n",
       "6151  binance_api  \n",
       "6152  binance_api  \n",
       "6153  binance_api  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_data.tail(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5581a273-8847-4a84-acd4-4b0d1d7cd917",
   "metadata": {},
   "source": [
    "## Development of a Preprocessor for time series\n",
    "\n",
    "### The first unoptimized version packed into a single class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4088c71e-e523-4cba-aeb6-8b47a9388756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from scipy.signal import find_peaks\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class PeriodicityEstimator(BaseEstimator): # removed TransformerMixin t.k transform not needed\n",
    "    def __init__(self, max_nlags_ratio=0.5, peak_prominence_threshold=0.1,\n",
    "                 period_threshold=0.8, min_period=2, max_period=None):\n",
    "        self.max_nlags_ratio = max_nlags_ratio\n",
    "        self.peak_prominence_threshold = peak_prominence_threshold\n",
    "        self.period_threshold = period_threshold\n",
    "        self.min_period = min_period\n",
    "        self.max_period = max_period\n",
    "        self.period_ = None # to store the found period\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "          # the fit method does nothing but save the passed parameters\n",
    "          # Training takes place in GridSearchCV\n",
    "            return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "          \"\"\"Finds the period in data X using object parameters.\"\"\"\n",
    "\n",
    "          period = self._find_period(X)\n",
    "          self.period_=period\n",
    "          return period\n",
    "\n",
    "\n",
    "    def _find_period(self, data):\n",
    "        n = len(data)\n",
    "        max_nlags = int(self.max_nlags_ratio * n)\n",
    "        if self.max_period is None:\n",
    "            max_period_loc = n // 2\n",
    "        else:\n",
    "            max_period_loc = self.max_period\n",
    "\n",
    "        acf_values = acf(data, nlags=max_nlags, fft=True)\n",
    "        peaks, _ = find_peaks(acf_values, prominence=self.peak_prominence_threshold)\n",
    "\n",
    "        valid_peaks = peaks[(peaks >= self.min_period) & (peaks <= max_period_loc)]\n",
    "\n",
    "        if not valid_peaks.size:\n",
    "            period = 0\n",
    "        else:\n",
    "            acf_peak_values = acf_values[valid_peaks]\n",
    "            strong_peaks = np.where(acf_peak_values >= self.period_threshold)[0]\n",
    "            if strong_peaks.size > 0:\n",
    "                period = valid_peaks[strong_peaks[0]]\n",
    "            else:\n",
    "                period = valid_peaks[np.argmax(acf_peak_values)]\n",
    "\n",
    "        return period\n",
    "\n",
    "\n",
    "class PeriodicityDetector(BaseEstimator):\n",
    "    def __init__(self, min_period=2, max_period=None, cv=5):\n",
    "        self.min_period = min_period\n",
    "        self.max_period = max_period\n",
    "        self.cv = cv\n",
    "        self.best_estimator_=None\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        param_grid = {\n",
    "            'max_nlags_ratio': [0.1, 0.25, 0.5],\n",
    "            'peak_prominence_threshold': [0.05, 0.1, 0.2],\n",
    "            'period_threshold': [0.5, 0.6, 0.7, 0.8],\n",
    "            'min_period': [self.min_period], # pass min_period and max_period into estimator\n",
    "            'max_period': [self.max_period]\n",
    "        }\n",
    "\n",
    "\n",
    "        estimator = PeriodicityEstimator()\n",
    "        grid_search = GridSearchCV(estimator, param_grid, scoring=self._score_period, cv=self.cv, n_jobs=-1)\n",
    "        grid_search.fit(X.to_numpy().reshape(-1, 1), y=[0] * len(X))\n",
    "\n",
    "        self.best_estimator_ = grid_search.best_estimator_\n",
    "        return self\n",
    "\n",
    "    def _score_period(self,y, period_ ,X=None ,**kwargs):\n",
    "        # period_ is an array of predictions for each Fad, we only need one\n",
    "        period = period_[0]\n",
    "\n",
    "        if period == 0:\n",
    "             return -np.inf\n",
    "\n",
    "        try:\n",
    "            from statsmodels.tsa.seasonal import STL\n",
    "            stl = STL(X, period=period)\n",
    "            res = stl.fit()\n",
    "            return -res.resid.var()\n",
    "        except:\n",
    "            return -np.inf\n",
    "\n",
    "    def predict(self,X):\n",
    "          return self.best_estimator_.predict(X)\n",
    "\n",
    "\n",
    "# example of using\n",
    "#index = pd.date_range('1/1/2000', periods=365 * 10, freq='D')\n",
    "#data = pd.Series(np.sin(2 * np.pi * index.dayofyear / 12) + np.random.randn(len(index)) / 5, index=index)\n",
    "#detector = PeriodicityDetector(cv=20)\n",
    "#detector.fit(data)\n",
    "#period = detector.predict(data)\n",
    "#print(f\"Optimal period: {period}\")\n",
    "#print(f\"Best parameters: {detector.best_estimator_.get_params()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0b231bf-5d7e-4a22-867d-022ad1c565f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesPreprocessorManager:\n",
    "    \"\"\"\n",
    "        Class for time series preprocessing.\n",
    "        \n",
    "        Main purpose:\n",
    "          - Prepare data for machine learning models and neural networks.\n",
    "          - Perform cleaning, missing value imputation, outlier removal, scaling,\n",
    "            decomposition, lag feature generation, and additional feature engineering.\n",
    "        \n",
    "        Usage example:\n",
    "            preprocessor = TimeSeriesPreprocessorManager(\n",
    "                interval='1d', \n",
    "                timestamp_column='Open_time', \n",
    "                target_column='Open', \n",
    "                scaler='robust', \n",
    "                lag_features=5\n",
    "            )\n",
    "            processed_df = preprocessor.process(raw_df)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, interval, timestamp_column='Open_time', target_column='Open', additional_columns=None, scaler='robust', lag_features=5):\n",
    "        \"\"\"\n",
    "        Initialize the time series preprocessing manager.\n",
    "\n",
    "        :param interval: Data interval (e.g., '1h', '12h', '1d', '3d', '1w', '1M').\n",
    "        :param timestamp_column: Name of the timestamp column (e.g., 'Open_time').\n",
    "        :param target_column: Main column for forecasting (e.g., 'Open').\n",
    "        :param additional_columns: List of additional columns to process.\n",
    "        :param scaler: Scaling type ('minmax', 'standard', 'robust', 'quantile', 'log').\n",
    "                       For financial data, 'robust' is recommended by default.\n",
    "        :param lag_features: Number of lag features to generate (if specific configurations are not set).\n",
    "        \"\"\"\n",
    "        self.interval = interval\n",
    "        self.timestamp_column = timestamp_column\n",
    "        self.target_column = target_column\n",
    "        self.additional_columns = additional_columns if additional_columns else []\n",
    "        self.scaler_type = scaler\n",
    "        self.lag_features = lag_features\n",
    "        self.original_data = None\n",
    "        \n",
    "\n",
    "    def analyze_series(self, data):\n",
    "        \"\"\"\n",
    "            Analyze time series to detect missing values, outliers, and assess stationarity.\n",
    "            \n",
    "            Performs:\n",
    "              - Count of missing values.\n",
    "              - Outlier detection using Z-score, IQR method, and MAD.\n",
    "              - Stationarity assessment using ADF test, KPSS test, rolling statistics stability analysis,\n",
    "                autocorrelation (lag 1), and Q-Q analysis.\n",
    "            \n",
    "            :param data: Pandas Series or array of time series values.\n",
    "            :return: Dictionary with analysis results.\n",
    "        \"\"\"\n",
    "        analysis = {}\n",
    "    \n",
    "        # Missing values\n",
    "        analysis['missing_values'] = data.isnull().sum()\n",
    "    \n",
    "        # Outliers: Z-score\n",
    "        z_scores = zscore(data.dropna())\n",
    "        analysis['zscore_outliers'] = int((np.abs(z_scores) > 3).sum())\n",
    "    \n",
    "        # Outliers: IQR method\n",
    "        q1 = data.quantile(0.25)\n",
    "        q3 = data.quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        analysis['iqr_outliers'] = int(((data < (q1 - 1.5 * iqr)) | (data > (q3 + 1.5 * iqr))).sum())\n",
    "    \n",
    "        # Outliers: MAD method\n",
    "        median_val = data.median()\n",
    "        mad = np.median(np.abs(data - median_val))\n",
    "        modified_z = 0.6745 * (data - median_val) / mad if mad != 0 else np.zeros_like(data)\n",
    "        analysis['mad_outliers'] = int((np.abs(modified_z) > 3.5).sum())\n",
    "    \n",
    "        # Stationarity: ADF test\n",
    "        adf_result = adfuller(data.dropna())\n",
    "        analysis['adf_pvalue'] = adf_result[1]\n",
    "        stationary_adf = adf_result[1] < 0.05\n",
    "    \n",
    "        # Stationarity: KPSS test\n",
    "        try:\n",
    "            kpss_result = kpss(data.dropna(), regression='c', nlags=\"auto\")\n",
    "            analysis['kpss_pvalue'] = kpss_result[1]\n",
    "            stationary_kpss = kpss_result[1] > 0.05  # Null hypothesis: series is stationary\n",
    "        except Exception as e:\n",
    "            analysis['kpss_pvalue'] = np.nan\n",
    "            stationary_kpss = False\n",
    "    \n",
    "        # Rolling statistics stability analysis (mean and std)\n",
    "        window = max(30, len(data) // 10)  # window size at least 30 observations\n",
    "        rolling_mean = data.rolling(window=window, min_periods=1).mean()\n",
    "        rolling_std = data.rolling(window=window, min_periods=1).std()\n",
    "        mean_cv = rolling_mean.std() / rolling_mean.mean() if rolling_mean.mean() != 0 else np.inf\n",
    "        std_cv = rolling_std.std() / rolling_std.mean() if rolling_std.mean() != 0 else np.inf\n",
    "        analysis['rolling_mean_cv'] = mean_cv\n",
    "        analysis['rolling_std_cv'] = std_cv\n",
    "    \n",
    "        # Autocorrelation at lag 1\n",
    "        autocorr_lag1 = data.autocorr(lag=1)\n",
    "        analysis['lag1_autocorrelation'] = autocorr_lag1\n",
    "    \n",
    "        # Q-Q analysis: calculate correlation between empirical and theoretical quantiles\n",
    "        qq = probplot(data.dropna(), dist=\"norm\", plot=None)\n",
    "        analysis['qq_correlation'] = qq[1][2]\n",
    "    \n",
    "        # Final stationarity assessment: series is considered stationary if:\n",
    "        #   - ADF confirms stationarity (p < 0.05),\n",
    "        #   - KPSS does not reject stationarity hypothesis (p > 0.05),\n",
    "        #   - Coefficients of variation for rolling mean and std are below threshold.\n",
    "        rolling_threshold = 0.2\n",
    "        self._is_stationary = (stationary_adf and stationary_kpss and \n",
    "                                  (mean_cv < rolling_threshold) and (std_cv < rolling_threshold))\n",
    "\n",
    "        analysis['stationary'] = self._is_stationary\n",
    "    \n",
    "        return analysis\n",
    "\n",
    "\n",
    "    def detrend_series(self, data, method='linear'):\n",
    "        \"\"\"\n",
    "        Remove trend from time series.\n",
    "        \n",
    "        Two methods:\n",
    "          - 'linear': Remove linear trend using polynomial regression.\n",
    "          - 'difference': Differentiate series (remove trend by computing differences).\n",
    "        \n",
    "        :param data: Pandas Series with time series.\n",
    "        :param method: String 'linear' or 'difference'.\n",
    "        :return: Tuple (detrended_series, trend_component), where detrended_series is the detrended series,\n",
    "                 and trend_component is the extracted trend component or initial value (for difference).\n",
    "        \"\"\"\n",
    "        if method == 'linear':\n",
    "            x = np.arange(len(data))\n",
    "            coeffs = np.polyfit(x, data.values, 1)\n",
    "            trend = np.polyval(coeffs, x)\n",
    "            detrended = data - trend\n",
    "            return detrended, pd.Series(trend, index=data.index)\n",
    "        elif method == 'difference':\n",
    "            detrended = data.diff().dropna()\n",
    "            # For inverse transformation with differencing, save the initial value\n",
    "            return detrended, data.iloc[0]\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported detrending method. Choose 'linear' or 'difference'.\")\n",
    "\n",
    "\n",
    "    def inverse_detrend(self, detrended_data, trend_component, method='linear'):\n",
    "        \"\"\"\n",
    "        Inverse transformation to restore trend component in time series.\n",
    "    \n",
    "        :param detrended_data: Pandas Series without trend.\n",
    "        :param trend_component: Trend component obtained during detrending.\n",
    "        :param method: Inverse transformation method ('linear' or 'difference').\n",
    "        :return: Restored series with trend.\n",
    "        \"\"\"\n",
    "        if method == 'linear':\n",
    "            return detrended_data + trend_component\n",
    "        elif method == 'difference':\n",
    "            # Inverse transformation for differencing: cumulative sum of differences plus initial value\n",
    "            return detrended_data.cumsum() + trend_component\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported inverse detrending method. Choose 'linear' or 'difference'.\")\n",
    "\n",
    "    \n",
    "    def fill_missing_values(self, data, method='linear'):\n",
    "        \"\"\"\n",
    "            Fill missing values in data using specified interpolation method.\n",
    "            \n",
    "            :param data: Pandas Series of time series.\n",
    "            :param method: Filling method ('linear', 'mean', etc.).\n",
    "            :return: Series with filled missing values.\n",
    "            Example:\n",
    "                filled_series = preprocessor.fill_missing_values(series, method='linear')\n",
    "        \"\"\"\n",
    "        return data.interpolate(method=method).bfill().ffill()\n",
    "\n",
    "\n",
    "    def _optimize_contamination(self, data, method='isolation_forest'):\n",
    "        \"\"\"\n",
    "            Optimize contamination parameter (outlier fraction) based on data analysis.\n",
    "            \n",
    "            :param data: Pandas Series of time series.\n",
    "            :param method: Method for outlier removal ('isolation_forest', 'lof', 'auto').\n",
    "            :return: Optimal contamination value.\n",
    "        \"\"\"\n",
    "        if not isinstance(data, pd.Series):\n",
    "            data = data[self.target_column]\n",
    "        \n",
    "        # Dynamic contamination range determination\n",
    "        iqr = data.quantile(0.75) - data.quantile(0.25)\n",
    "        lower_bound = data.quantile(0.25) - 1.5 * iqr\n",
    "        upper_bound = data.quantile(0.75) + 1.5 * iqr\n",
    "        estimated_outliers = ((data < lower_bound) | (data > upper_bound)).sum()\n",
    "        estimated_contamination = max(estimated_outliers / len(data), 0.001)\n",
    "    \n",
    "        # Generate logarithmic contamination range\n",
    "        contamination_range = np.geomspace(max(0.0001, estimated_contamination / 2), \n",
    "                                           min(0.51, estimated_contamination * 2), 10)\n",
    "\n",
    "        best_contamination = 0.001\n",
    "        best_score = float('inf')\n",
    "    \n",
    "        from joblib import Parallel, delayed\n",
    "    \n",
    "        def evaluate_contamination(contamination):\n",
    "            try:\n",
    "                if method == 'isolation_forest':\n",
    "                    processed_data = self.remove_outliers_with_isolation_forest(data.copy(), contamination)\n",
    "                elif method == 'lof':\n",
    "                    processed_data = self.remove_outliers_with_lof(data.copy(), contamination)\n",
    "                else:\n",
    "                    raise ValueError(\"Unsupported method for contamination optimization.\")\n",
    "    \n",
    "                # Quality assessment: residual autocorrelation\n",
    "                residual_diff = data - processed_data\n",
    "                score = np.mean(np.abs(residual_diff))\n",
    "    \n",
    "                # Alternative metric: autocorrelation\n",
    "                autocorr_score = residual_diff.autocorr()\n",
    "                final_score = score * (1 - abs(autocorr_score))\n",
    "    \n",
    "                return contamination, final_score\n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating contamination {contamination}: {e}\")\n",
    "                return contamination, float('inf')\n",
    "    \n",
    "        results = Parallel(n_jobs=-1)(delayed(evaluate_contamination)(cont) for cont in contamination_range)\n",
    "    \n",
    "        # Select best contamination\n",
    "        for contamination, score in results:\n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_contamination = contamination\n",
    "    \n",
    "        return best_contamination\n",
    "\n",
    "\n",
    "    def remove_outliers_with_isolation_forest(self, data, contamination=None):\n",
    "        \"\"\"\n",
    "            Remove outliers using Isolation Forest with automatic contamination tuning.\n",
    "    \n",
    "            :param data: Pandas Series of time series.\n",
    "            :param contamination: Expected fraction of outliers in data. If None, calculated automatically.\n",
    "            :return: Series with outliers replaced by median.\n",
    "        \"\"\"\n",
    "        if contamination is None:\n",
    "            # Automatic contamination selection via cross-validation\n",
    "            contamination = self._optimize_contamination(data, method='isolation_forest')\n",
    "        reshaped_data = data.values.reshape(-1, 1)\n",
    "        isol_forest = IsolationForest(contamination=contamination)\n",
    "        outliers = isol_forest.fit_predict(reshaped_data)\n",
    "  \n",
    "        median_value = np.median(data[~np.isin(outliers, -1)])\n",
    "        data[outliers == -1] = median_value\n",
    "        return data\n",
    "\n",
    "\n",
    "    def remove_outliers_with_lof(self, data, contamination=None, n_neighbors=None):\n",
    "        \"\"\"\n",
    "            Remove outliers using Local Outlier Factor (LOF).\n",
    "    \n",
    "            :param data: Pandas Series of time series.\n",
    "            :param contamination: Expected fraction of outliers in data.\n",
    "            :return: Series with outliers replaced by median.\n",
    "        \"\"\"\n",
    "        if contamination is None:\n",
    "            # Automatic contamination selection via cross-validation\n",
    "            contamination = self._optimize_contamination(data, method='lof')\n",
    "\n",
    "        # Automatic n_neighbors selection if not set\n",
    "        if n_neighbors is None:\n",
    "            # Example: choose n_neighbors as 5% of total observations,\n",
    "            # but not less than 5 and not more than 20\n",
    "            n_neighbors = min(20, max(5, int(len(data) * 0.05)))\n",
    "\n",
    "        reshaped_data = data.values.reshape(-1, 1)\n",
    "        lof = LocalOutlierFactor(contamination=contamination, n_neighbors=n_neighbors)\n",
    "        outliers = lof.fit_predict(reshaped_data)\n",
    "        median_value = np.median(data[outliers == 1])\n",
    "        data[outliers == -1] = median_value\n",
    "        return data\n",
    "\n",
    "\n",
    "    def decompose_and_remove_outliers(self, data, method='isolation_forest', contamination=None):\n",
    "        \"\"\"\n",
    "            Combined method: time series decomposition and outlier removal in residuals.\n",
    "    \n",
    "            :param data: Pandas Series of time series.\n",
    "            :param method: Outlier removal method ('isolation_forest', 'lof').\n",
    "            :param contamination: Expected outlier fraction. If None, calculated automatically.\n",
    "            :return: DataFrame with updated components (trend, seasonality, residual).\n",
    "        \"\"\"\n",
    "        trend, seasonal, residual = self.decompose_series(data, method='STL')\n",
    "\n",
    "        if method == 'isolation_forest':\n",
    "            residual_cleaned = self.remove_outliers_with_isolation_forest(residual, contamination)\n",
    "        elif method == 'lof':\n",
    "            residual_cleaned = self.remove_outliers_with_lof(residual, contamination)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported method for outlier removal.\")\n",
    "\n",
    "        # Reconstruct time series\n",
    "        cleaned_series = trend + seasonal + residual_cleaned\n",
    "\n",
    "        return cleaned_series \n",
    "        #pd.DataFrame({\n",
    "        #    f'{self.timestamp_column}': data.index,\n",
    "        #    f'{self.target_column}': cleaned_series\n",
    "        #})\n",
    "\n",
    "\n",
    "    def remove_outliers(self, data, method='combined', contamination=None):\n",
    "        \"\"\"\n",
    "            Select outlier removal method (Isolation Forest, LOF, or combined method).\n",
    "    \n",
    "            :param data: Pandas Series of time series.\n",
    "            :param method: Outlier removal method ('isolation_forest', 'lof', 'combined').\n",
    "            :param contamination: Expected outlier fraction in data. If None, calculated automatically.\n",
    "            :return: Series with outliers replaced by median or DataFrame (for combined method).\n",
    "        \"\"\"\n",
    "        if method == 'combined':\n",
    "            # Automatic method selection\n",
    "            remove_outliers_method = 'isolation_forest' if len(data) > 1000 else 'lof'\n",
    "            return self.decompose_and_remove_outliers(data.copy(), method=remove_outliers_method, contamination=contamination)\n",
    "        elif method == 'isolation_forest':\n",
    "            return self.remove_outliers_with_isolation_forest(data.copy(), contamination)\n",
    "        elif method == 'lof':\n",
    "            return self.remove_outliers_with_lof(data.copy(), contamination)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported outlier removal method.\")\n",
    "\n",
    "\n",
    "    def determine_period(self, data):\n",
    "        \"\"\"\n",
    "            Determine period (seasonality) of time series using multiple methods.\n",
    "        \n",
    "            Uses:\n",
    "              - PeriodicityDetector,\n",
    "              - Autocorrelation (ACF),\n",
    "              - Spectral analysis (FFT).\n",
    "            \n",
    "            If period not found, applies heuristics based on self.interval.\n",
    "            \n",
    "            :param data: Pandas Series with time series.\n",
    "            :return: Tuple (period, period_candidates)\n",
    "        \"\"\"\n",
    "        detector = PeriodicityDetector(min_period=2, max_period=None, cv=10)\n",
    "        detector.fit(data)\n",
    "        period_detector = detector.predict(data)\n",
    "\n",
    "        period_candidates = []\n",
    "        if period_detector > 1:\n",
    "            print(f\"[LOG] Applying PeriodicityDetector!!! {period_detector}\")\n",
    "            period_candidates.append(period_detector)\n",
    "\n",
    "        # Autocorrelation method (ACF)\n",
    "        acf_vals = acf(data.dropna(), nlags=min(100, len(data)//2))\n",
    "        for lag, val in enumerate(acf_vals[1:], start=1):\n",
    "            if val > 0.3:\n",
    "                print(f\"[LOG] Applying ACF!!! {lag}\")\n",
    "                period_candidates.append(lag)\n",
    "                break\n",
    "\n",
    "        # Spectral analysis (FFT)\n",
    "        fft_data = data.dropna().values\n",
    "        fft_vals = np.abs(np.fft.rfft(fft_data))\n",
    "        freqs = np.fft.rfftfreq(len(fft_data), d=1)\n",
    "        if len(fft_vals) > 1:\n",
    "            index_candidate = np.argmax(fft_vals[1:]) + 1  # get index excluding zero frequency\n",
    "            # If obtained index exceeds freqs array bounds, correct it\n",
    "            if index_candidate >= len(freqs):\n",
    "                index_candidate = len(freqs) - 1\n",
    "            dominant_freq = freqs[index_candidate]\n",
    "            default_max_period = {\n",
    "                '1h': 168,    # 24*7 - week for hourly data\n",
    "                '12h': 60,    # approximately 30 days\n",
    "                '1d': 182,    # approximately half year for daily data\n",
    "                '3d': 60,     # approximately 180 days/3 ≈ 60 observations for 3-day interval\n",
    "                '1w': 52,     # 52 weeks (year) for weekly data\n",
    "                '1M': 12      # 12 months for monthly data\n",
    "            }\n",
    "            max_allowed_period = default_max_period.get(self.interval, len(data)/2)\n",
    "            if dominant_freq > 0:\n",
    "                period_fft = int(round(1 / dominant_freq))\n",
    "                if period_fft < max_allowed_period:\n",
    "                    print(f\"[LOG] Applying FFT!!! {period_fft}\")\n",
    "                    period_candidates.append(period_fft)\n",
    "                else:\n",
    "                    print(f\"[LOG] Ignoring FFT candidate: {period_fft} (exceeds threshold)\")\n",
    "\n",
    "        # If candidates found, take median\n",
    "        if period_candidates:\n",
    "            period = int(np.round(np.median(period_candidates)))\n",
    "        else:\n",
    "            period = 0  # If nothing found\n",
    "\n",
    "        \n",
    "        # Fallback: use heuristic values based on self.interval\n",
    "        if period < 2:\n",
    "            print(f\"[LOG] Applying Fallback!!!\")\n",
    "            default_periods = {\n",
    "                '1h': 24,\n",
    "                '12h': 14,  # option for 12-hour data\n",
    "                '1d': 7,\n",
    "                '3d': 7,\n",
    "                '1w': 52,\n",
    "                '1M': 12\n",
    "            }\n",
    "            period = default_periods.get(self.interval, 1)\n",
    "        \n",
    "        print(f\"[LOG] Determined period: {period}, candidates: {period_candidates}\")\n",
    "        return period, period_candidates\n",
    "\n",
    "\n",
    "    def get_seasonality_config(self, interval, data_length):\n",
    "        \"\"\"\n",
    "            Automatic seasonality parameter configuration based on interval and data length.\n",
    "            Used in decompose_series()/`Prophet`.\n",
    "            \n",
    "            :param interval: Data interval (e.g., '1h', '1d', etc.).\n",
    "            :param data_length: Number of observations.\n",
    "            :return: Dictionary with seasonality configuration.\n",
    "        \"\"\"\n",
    "        configs = {\n",
    "            '1h': {'daily': True, 'weekly': False, 'yearly': False},\n",
    "            '12h': {'daily': True, 'weekly': True, 'yearly': False},\n",
    "            '1d': {'daily': False, 'weekly': True, 'yearly': True},\n",
    "            '3d': {'daily': False, 'weekly': True, 'yearly': True},\n",
    "            '1w': {'daily': False, 'weekly': False, 'yearly': True},\n",
    "            '1M': {'daily': False, 'weekly': False, 'yearly': True}\n",
    "        }\n",
    "        config = configs.get(interval, {'daily': False, 'weekly': False, 'yearly': False})\n",
    "        if data_length < 365:  # Disable yearly seasonality for short series\n",
    "            config['yearly'] = False\n",
    "        return config\n",
    "\n",
    "\n",
    "    def decompose_series(self, data, method='auto', freq=None, model='auto'):\n",
    "        \"\"\"\n",
    "        Automatic time series decomposition with method selection option.\n",
    "        \"\"\"\n",
    "        if not isinstance(data.index, pd.DatetimeIndex):\n",
    "            raise ValueError(\"Data must have a DatetimeIndex for seasonal decomposition.\")\n",
    "    \n",
    "        adf_pvalue = adfuller(data.dropna())[1]\n",
    "        model_type = 'additive' if adf_pvalue < 0.05 else 'multiplicative'\n",
    "        \n",
    "        period_median, period_candidates = self.determine_period(data)\n",
    "        method = self._determine_method_auto(data, period_median, period_candidates) if method == 'auto' else method\n",
    "    \n",
    "        decomposition_methods = {\n",
    "            'STL': self._decompose_stl,\n",
    "            'TBATS': self._decompose_tbats,\n",
    "            'SSA': self._decompose_ssa,\n",
    "            'fourier': self._decompose_fourier,\n",
    "            'prophet': self._decompose_prophet\n",
    "        }\n",
    "    \n",
    "        if method in decomposition_methods:\n",
    "            return decomposition_methods[method](data, period_median, period_candidates, model_type)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported decomposition method: {method}\")\n",
    "\n",
    "\n",
    "    def _determine_method_auto(self, data, period, candidates):\n",
    "        if period < 2:\n",
    "            return 'SSA'\n",
    "        methods = ['STL', 'SSA', 'fourier']\n",
    "        quality = {}\n",
    "        for method in methods:\n",
    "            result = getattr(self, f'_decompose_{method.lower()}')(data, period, candidates)\n",
    "            quality[method] = np.nanmean(np.abs(result[2].dropna()))  # Assuming the residual is the third element\n",
    "        return min(quality, key=quality.get)\n",
    "\n",
    "\n",
    "    def _decompose_stl(self, data, period, candidates, model_type):\n",
    "        print('[LOG] STL method selected')\n",
    "        stl = STL(data, period=period, robust=True) # , seasonal=model_type\n",
    "        result = stl.fit()\n",
    "        return result.trend, result.seasonal, result.resid\n",
    "\n",
    "    \n",
    "    def _decompose_tbats(self, data, period, candidates, model_type):\n",
    "        print('[LOG] TBATS method selected')\n",
    "        from tbats import TBATS\n",
    "        seasonal_periods = period_candidates if len(period_candidates) > 1 else [period_median]\n",
    "        if self.interval == '1h':\n",
    "            seasonal_periods = [24, 168]\n",
    "        print(f\"[LOG] Using seasonal periods for TBATS: {seasonal_periods}\")\n",
    "        estimator = TBATS(seasonal_periods=seasonal_periods)\n",
    "        tbats_model = estimator.fit(data)\n",
    "        fitted = tbats_model.y_hat  # fitted values\n",
    "        try:\n",
    "            seasonal_array = np.sum(tbats_model.seasonal_components_, axis=1)\n",
    "        except AttributeError:\n",
    "            seasonal_array = np.zeros_like(fitted)\n",
    "        trend_array = fitted - seasonal_array\n",
    "        resid_array = data.values.flatten() - fitted\n",
    "        trend = pd.Series(trend_array, index=data.index)\n",
    "        seasonal = pd.Series(seasonal_array, index=data.index)\n",
    "        resid = pd.Series(resid_array, index=data.index)\n",
    "        return trend, seasonal, resid\n",
    "        \n",
    "\n",
    "    def _decompose_ssa(self, data, period, candidates, model_type):\n",
    "        print('[LOG] SSA method selected')\n",
    "        return self.ssa_decompose(data, window_length=period, variance_threshold=0.9)\n",
    "\n",
    "\n",
    "    def _decompose_fourier(self, data, period, candidates, model_type):\n",
    "        print('[LOG] FOURIER method selected')\n",
    "        return self.fourier_decompose(data, period=period)\n",
    "\n",
    "    \n",
    "    def _decompose_prophet(self, data, period, candidates, model_type):\n",
    "        print('[LOG] Prophet method selected')\n",
    "        df = pd.DataFrame(data)\n",
    "        df.reset_index(inplace=True)\n",
    "        df.columns = ['ds', 'y']\n",
    "        data_length = len(df)\n",
    "        seasonality_config = self.get_seasonality_config(self.interval, data_length)\n",
    "        components = [comp for comp, flag in seasonality_config.items() if flag]\n",
    "        from fbprophet import Prophet\n",
    "        model_prophet = Prophet(\n",
    "            daily_seasonality=seasonality_config.get('daily', False),\n",
    "            weekly_seasonality=seasonality_config.get('weekly', False),\n",
    "            yearly_seasonality=seasonality_config.get('yearly', False),\n",
    "            changepoint_prior_scale=0.05,\n",
    "            seasonality_prior_scale=10,\n",
    "            seasonality_mode='multiplicative'\n",
    "        )\n",
    "        model_prophet.fit(df)\n",
    "        forecast = model_prophet.predict(df)\n",
    "        trend_series = pd.Series(forecast['trend'].values, index=data.index)\n",
    "        seasonal_series = pd.Series(forecast[components].sum(axis=1).values, index=data.index)\n",
    "        resid_series = pd.Series((df['y'] - forecast['trend'] - forecast[components].sum(axis=1)).values, index=data.index)\n",
    "        return trend_series, seasonal_series, resid_series\n",
    "\n",
    "\n",
    "    # DEPRECATED --> TODO: REMOVE \n",
    "    def decompose_series_old(self, data, method='auto', freq=None, model='auto'):\n",
    "        \"\"\"\n",
    "        Decompose time series into trend, seasonality, and residual.\n",
    "        Automatically selects decomposition method (STL, SSA, FOURIER, TBATS, Prophet) based on data analysis.\n",
    "    \n",
    "        Parameters:\n",
    "          :param data   : Pandas Series with DatetimeIndex.\n",
    "          :param method : Decomposition method ('auto', 'STL', 'TBATS', 'SSA', 'fourier', 'prophet').\n",
    "                          With 'auto', automatic evaluation between STL, SSA, and FOURIER occurs.\n",
    "          :param freq: Frequency if specified.\n",
    "          :param model: Model type ('additive', 'multiplicative', or 'auto').\n",
    "    \n",
    "        :return: Tuple (trend, seasonal, residual) – time series components as pandas Series.\n",
    "        \"\"\"\n",
    "        # Index validation\n",
    "        if not isinstance(data.index, pd.DatetimeIndex):\n",
    "            raise ValueError(\"Data must have a DatetimeIndex for seasonal decomposition.\")\n",
    "    \n",
    "        # Automatic model type selection (additive/multiplicative)\n",
    "        if model == 'auto':\n",
    "            adf_pvalue = adfuller(data.dropna())[1]\n",
    "            model_type = 'additive' if adf_pvalue < 0.05 else 'multiplicative'\n",
    "            print(f\"[LOG] ADF p-value = {adf_pvalue:.4f}, selected model: {model_type}\")\n",
    "        else:\n",
    "            model_type = model\n",
    "    \n",
    "        # Determine period and candidate values\n",
    "        period_median, period_candidates = self.determine_period(data)\n",
    "        print(f\"[LOG] Determined period (median): {period_median}\")\n",
    "        print(f\"[LOG] Period candidates: {period_candidates}\")\n",
    "    \n",
    "        # If period insufficient, use SSA by default\n",
    "        if period_median < 2:\n",
    "            if method == 'auto':\n",
    "                print(\"[LOG] Period less than 2, switching to SSA\")\n",
    "                method = 'SSA'\n",
    "    \n",
    "        # If method explicitly specified, execute corresponding branch\n",
    "        if method != 'auto':\n",
    "            if method.upper() == 'STL' and period_median > 1:\n",
    "                print('[LOG] STL method explicitly selected')\n",
    "                stl = STL(data, period=period_median, robust=True)\n",
    "                result = stl.fit()\n",
    "                return result.trend, result.seasonal, result.resid\n",
    "    \n",
    "            elif method.upper() == 'TBATS':\n",
    "                print('[LOG] TBATS method explicitly selected')\n",
    "                from tbats import TBATS\n",
    "                seasonal_periods = period_candidates if len(period_candidates) > 1 else [period_median]\n",
    "                if self.interval == '1h':\n",
    "                    seasonal_periods = [24, 168]\n",
    "                print(f\"[LOG] Using seasonal periods for TBATS: {seasonal_periods}\")\n",
    "                estimator = TBATS(seasonal_periods=seasonal_periods)\n",
    "                tbats_model = estimator.fit(data)\n",
    "                fitted = tbats_model.y_hat  # fitted values\n",
    "                try:\n",
    "                    seasonal_array = np.sum(tbats_model.seasonal_components_, axis=1)\n",
    "                except AttributeError:\n",
    "                    seasonal_array = np.zeros_like(fitted)\n",
    "                trend_array = fitted - seasonal_array\n",
    "                resid_array = data.values.flatten() - fitted\n",
    "                trend = pd.Series(trend_array, index=data.index)\n",
    "                seasonal = pd.Series(seasonal_array, index=data.index)\n",
    "                resid = pd.Series(resid_array, index=data.index)\n",
    "                return trend, seasonal, resid\n",
    "    \n",
    "            elif method.upper() == 'SSA':\n",
    "                print('[LOG] SSA method explicitly selected')\n",
    "                return self.ssa_decompose(data, window_length=period_median, variance_threshold=0.9)\n",
    "    \n",
    "            elif method.lower() == 'fourier':\n",
    "                print('[LOG] FOURIER method explicitly selected')\n",
    "                return self.fourier_decompose(data, period=period_median)\n",
    "    \n",
    "            else:\n",
    "                print('[LOG] Prophet method explicitly selected')\n",
    "                df = pd.DataFrame(data)\n",
    "                df.reset_index(inplace=True)\n",
    "                df.columns = ['ds', 'y']\n",
    "                data_length = len(df)\n",
    "                seasonality_config = self.get_seasonality_config(self.interval, data_length)\n",
    "                components = [comp for comp, flag in seasonality_config.items() if flag]\n",
    "                from fbprophet import Prophet\n",
    "                model_prophet = Prophet(\n",
    "                    daily_seasonality=seasonality_config.get('daily', False),\n",
    "                    weekly_seasonality=seasonality_config.get('weekly', False),\n",
    "                    yearly_seasonality=seasonality_config.get('yearly', False),\n",
    "                    changepoint_prior_scale=0.05,\n",
    "                    seasonality_prior_scale=10,\n",
    "                    seasonality_mode='multiplicative'\n",
    "                )\n",
    "                model_prophet.fit(df)\n",
    "                forecast = model_prophet.predict(df)\n",
    "                trend_series = pd.Series(forecast['trend'].values, index=data.index)\n",
    "                seasonal_series = pd.Series(forecast[components].sum(axis=1).values, index=data.index)\n",
    "                resid_series = pd.Series((df['y'] - forecast['trend'] - forecast[components].sum(axis=1)).values, index=data.index)\n",
    "                return trend_series, seasonal_series, resid_series\n",
    "    \n",
    "        # Auto mode: evaluate candidates between STL, SSA, and Fourier\n",
    "        if method == 'auto':\n",
    "            candidates = {}\n",
    "            quality = {}  # quality assessed by mean absolute value of residuals (MAE)\n",
    "            \n",
    "            # Try STL\n",
    "            if period_median > 1:\n",
    "                try:\n",
    "                    print(\"[LOG] Trying STL\")\n",
    "                    stl_result = STL(data, period=period_median, robust=True).fit()\n",
    "                    candidates['STL'] = (stl_result.trend, stl_result.seasonal, stl_result.resid)\n",
    "                    quality['STL'] = np.nanmean(np.abs(stl_result.resid.dropna()))\n",
    "                    print(f\"[LOG] STL: residual MAE = {quality['STL']:.4f}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"[ERROR] STL error: {e}\")\n",
    "            # Try SSA\n",
    "            try:\n",
    "                print(\"[LOG] Trying SSA\")\n",
    "                ssa_result = self.ssa_decompose(data, window_length=period_median, variance_threshold=0.9)\n",
    "                candidates['SSA'] = ssa_result\n",
    "                quality['SSA'] = np.nanmean(np.abs(ssa_result[2].dropna()))\n",
    "                print(f\"[LOG] SSA: residual MAE = {quality['SSA']:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] SSA error: {e}\")\n",
    "            # Try Fourier regression\n",
    "            try:\n",
    "                print(\"[LOG] Trying FOURIER\")\n",
    "                fourier_result = self.fourier_decompose(data, period=period_median)\n",
    "                candidates['fourier'] = fourier_result\n",
    "                quality['fourier'] = np.nanmean(np.abs(fourier_result[2].dropna()))\n",
    "                print(f\"[LOG] FOURIER: residual MAE = {quality['fourier']:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] FOURIER error: {e}\")\n",
    "    \n",
    "            if not quality:\n",
    "                raise ValueError(\"Could not perform decomposition with any method in auto mode.\")\n",
    "    \n",
    "            # Priority: if STL available and its MAE not worse than best candidate (within 10%), choose STL.\n",
    "            best_quality = min(quality.values())\n",
    "            if 'STL' in quality and quality['STL'] <= best_quality * 1.1:\n",
    "                chosen = 'STL'\n",
    "            else:\n",
    "                chosen = min(quality, key=quality.get)\n",
    "    \n",
    "            print(f\"[LOG] Selected method: {chosen} (residual MAE = {quality[chosen]:.4f})\")\n",
    "            return candidates[chosen]\n",
    "        \n",
    "\n",
    "    def fourier_decompose(self, data, period):\n",
    "        \"\"\"\n",
    "            Decompose time series using Fourier regression.\n",
    "            \n",
    "            :param data: Pandas Series with time series.\n",
    "            :param period: Determined period.\n",
    "            :return: Tuple (trend, seasonal, residual) – series components.\n",
    "        \"\"\"\n",
    "        # Convert data to one-dimensional Series\n",
    "        data_series = pd.Series(data.values.flatten(), index=data.index)\n",
    "        df = pd.DataFrame({'y': data_series.values}, index=data_series.index)\n",
    "        df['t'] = np.arange(len(df))\n",
    "        best_aic = np.inf\n",
    "        best_model = None\n",
    "        best_X = None\n",
    "        max_k = 5  # maximum number of harmonics\n",
    "        for k in range(1, max_k + 1):\n",
    "            df_temp = df.copy()\n",
    "            for i in range(1, k + 1):\n",
    "                df_temp[f'sin_{i}'] = np.sin(2 * np.pi * i * df_temp['t'] / period)\n",
    "                df_temp[f'cos_{i}'] = np.cos(2 * np.pi * i * df_temp['t'] / period)\n",
    "            X = df_temp.drop(columns='y')\n",
    "            X = sm.add_constant(X)\n",
    "            model_fit = sm.OLS(df_temp['y'], X).fit()\n",
    "            if model_fit.aic < best_aic:\n",
    "                best_aic = model_fit.aic\n",
    "                best_model = model_fit\n",
    "                best_X = X.copy()\n",
    "        params = best_model.params\n",
    "        # 'const' and 't' describe trend, rest - seasonality\n",
    "        trend_values = best_X[['const', 't']].dot(params[['const', 't']])\n",
    "        seasonal_values = best_X.drop(columns=['const', 't']).dot(params.drop(labels=['const', 't']))\n",
    "        trend_series = pd.Series(trend_values, index=df.index)\n",
    "        seasonal_series = pd.Series(seasonal_values, index=df.index)\n",
    "        resid_series = data_series - trend_series - seasonal_series\n",
    "        return trend_series, seasonal_series, resid_series\n",
    "\n",
    "    \n",
    "    def ssa_decompose(self, data, window_length=None, variance_threshold=0.9):\n",
    "        \"\"\"\n",
    "            Decompose time series using Singular Spectrum Analysis (SSA).\n",
    "            \n",
    "            :param data: Pandas Series with time series.\n",
    "            :param window_length: Window length for trajectory matrix formation.\n",
    "            :param variance_threshold: Cumulative variance threshold for component selection.\n",
    "            :return: Tuple (trend, seasonal, residual) – series components.\n",
    "        \"\"\"\n",
    "        # Convert series to numpy array\n",
    "        series = data.values.flatten() if isinstance(data, pd.Series) else np.array(data).flatten()\n",
    "        N = len(series)\n",
    "        # Determine window length: if window_length not set, use period or N//2\n",
    "        if window_length is None:\n",
    "            period = self.determine_period(data)\n",
    "            window_length = period if period > 1 else N // 2\n",
    "    \n",
    "        K = N - window_length + 1\n",
    "        # Form trajectory (Hankel) matrix\n",
    "        X = np.empty((window_length, K))\n",
    "        for i in range(window_length):\n",
    "            X[i] = series[i:i+K]\n",
    "        # Singular value decomposition\n",
    "        U, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "        total_variance = np.sum(s**2)\n",
    "        explained_variance = np.cumsum(s**2) / total_variance\n",
    "        # Determine number of components needed to explain given variance threshold\n",
    "        r = np.searchsorted(explained_variance, variance_threshold) + 1\n",
    "    \n",
    "        # Reconstruct matrix using r components\n",
    "        X_reconstructed = np.zeros_like(X)\n",
    "        for i in range(r):\n",
    "            X_reconstructed += s[i] * np.outer(U[:, i], Vt[i, :])\n",
    "        # Diagonal averaging (reconstruction via Hankelization)\n",
    "        reconstructed = np.zeros(N)\n",
    "        counts = np.zeros(N)\n",
    "        for i in range(window_length):\n",
    "            for j in range(K):\n",
    "                reconstructed[i+j] += X_reconstructed[i, j]\n",
    "                counts[i+j] += 1\n",
    "        reconstructed /= counts\n",
    "    \n",
    "        # Group components to extract trend and seasonality:\n",
    "        # Hypothesis: first 2 components (if available) represent trend, rest - seasonality.\n",
    "        if r >= 2:\n",
    "            X_trend = s[0] * np.outer(U[:, 0], Vt[0, :])\n",
    "            X_trend += s[1] * np.outer(U[:, 1], Vt[1, :])\n",
    "            trend_reconstructed = np.zeros(N)\n",
    "            counts_trend = np.zeros(N)\n",
    "            for i in range(window_length):\n",
    "                for j in range(K):\n",
    "                    trend_reconstructed[i+j] += X_trend[i, j]\n",
    "                    counts_trend[i+j] += 1\n",
    "            trend_reconstructed /= counts_trend\n",
    "            seasonal_reconstructed = reconstructed - trend_reconstructed\n",
    "        else:\n",
    "            trend_reconstructed = reconstructed\n",
    "            seasonal_reconstructed = np.zeros_like(reconstructed)\n",
    "        residual = series - reconstructed\n",
    "    \n",
    "        # Convert results back to Pandas Series\n",
    "        if hasattr(data, 'index'):\n",
    "            trend_reconstructed = pd.Series(trend_reconstructed, index=data.index)\n",
    "            seasonal_reconstructed = pd.Series(seasonal_reconstructed, index=data.index)\n",
    "            residual = pd.Series(residual, index=data.index)\n",
    "        \n",
    "        return trend_reconstructed, seasonal_reconstructed, residual\n",
    "    \n",
    "\n",
    "    def scale_data(self, data):\n",
    "        \"\"\"\n",
    "            Scale data using selected algorithm.\n",
    "            \n",
    "            Supported algorithms:\n",
    "              - 'minmax': MinMaxScaler\n",
    "              - 'standard': StandardScaler\n",
    "              - 'robust': RobustScaler (robust to outliers)\n",
    "              - 'quantile': QuantileTransformer (converts distribution to normal)\n",
    "              - 'log': Logarithmic scaling (np.log1p)\n",
    "            \n",
    "            Key statistics logged before and after scaling.\n",
    "            \n",
    "            :param data: Pandas Series of time series.\n",
    "            :return: One-dimensional array of scaled data.\n",
    "        \"\"\"\n",
    "        # Log initial data statistics\n",
    "        initial_stats = {\n",
    "            'min': data.min(),\n",
    "            'max': data.max(),\n",
    "            'mean': data.mean(),\n",
    "            'std': data.std(),\n",
    "            'median': data.median(),\n",
    "            'IQR': data.quantile(0.75) - data.quantile(0.25)\n",
    "        }\n",
    "        print(\"[LOG] Initial data stats:\", initial_stats)\n",
    "    \n",
    "        reshaped_data = data.values.reshape(-1, 1)\n",
    "        \n",
    "        # Select scaling algorithm based on self.scaler_type parameter\n",
    "        if self.scaler_type == 'minmax':\n",
    "            from sklearn.preprocessing import MinMaxScaler\n",
    "            scaler = MinMaxScaler()\n",
    "        elif self.scaler_type == 'standard':\n",
    "            from sklearn.preprocessing import StandardScaler\n",
    "            scaler = StandardScaler()\n",
    "        elif self.scaler_type == 'robust':\n",
    "            from sklearn.preprocessing import RobustScaler\n",
    "            scaler = RobustScaler()\n",
    "        elif self.scaler_type == 'quantile':\n",
    "            from sklearn.preprocessing import QuantileTransformer\n",
    "            scaler = QuantileTransformer(output_distribution='normal')\n",
    "        elif self.scaler_type == 'log':\n",
    "            # Logarithmic scaling: note that data should be positive\n",
    "            scaled_data = np.log1p(reshaped_data)\n",
    "            self.custom_inverse = lambda x: np.expm1(x)\n",
    "            scaled_data = scaled_data.flatten()\n",
    "            # Log statistics after scaling\n",
    "            log_stats = {\n",
    "                'min': scaled_data.min(),\n",
    "                'max': scaled_data.max(),\n",
    "                'mean': scaled_data.mean(),\n",
    "                'std': scaled_data.std()\n",
    "            }\n",
    "            print(\"[LOG] Data after log scaling stats:\", log_stats)\n",
    "            return scaled_data\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported scaler type. Choose from: 'minmax', 'standard', 'robust', 'quantile', 'log'.\")\n",
    "    \n",
    "        # Apply selected scaler\n",
    "        scaled_data = scaler.fit_transform(reshaped_data).flatten()\n",
    "    \n",
    "        # Log statistics after scaling\n",
    "        after_stats = {\n",
    "            'min': scaled_data.min(),\n",
    "            'max': scaled_data.max(),\n",
    "            'mean': scaled_data.mean(),\n",
    "            'std': scaled_data.std()\n",
    "        }\n",
    "        print(\"[LOG] Scaled data stats:\", after_stats)\n",
    "    \n",
    "        # Save scaler for inverse transformation\n",
    "        self.scaler = scaler\n",
    "        return scaled_data\n",
    "\n",
    "\n",
    "    def inverse_scale(self, scaled_data):\n",
    "        \"\"\"\n",
    "            Inverse transformation of scaled data to original scale.\n",
    "            \n",
    "            Supports inverse transformation for logarithmic scaling via custom_inverse.\n",
    "            \n",
    "            :param scaled_data: Scaled data (numpy array).\n",
    "            :return: Data in original scale (numpy array).\n",
    "        \"\"\"\n",
    "        if self.scaler_type == 'log' and hasattr(self, 'custom_inverse'):\n",
    "            reshaped_data = scaled_data.reshape(-1, 1)\n",
    "            inversed = self.custom_inverse(reshaped_data)\n",
    "            return inversed.flatten()\n",
    "        else:\n",
    "            reshaped_data = scaled_data.reshape(-1, 1)\n",
    "            return self.scaler.inverse_transform(reshaped_data).flatten()\n",
    "\n",
    "    \n",
    "    def add_features(self, df, windows=None, holiday_country='US'):\n",
    "        \"\"\"\n",
    "        Add unified feature set to time series DataFrame.\n",
    "        \n",
    "        Feature set includes:\n",
    "          - Lag features (calculated on scaled column).\n",
    "          - Rolling statistics over multiple windows (mean, standard deviation, min,\n",
    "            max, median, skewness, kurtosis).\n",
    "          - Derivative features: first and second derivatives of series.\n",
    "          - Rolling trend: trend slope calculated via linear regression over window.\n",
    "          - Financial indicators: returns (pct_change) and volatility (rolling std of returns).\n",
    "          - Holiday flag: boolean is_holiday feature, True if date is a holiday for specified country.\n",
    "        \n",
    "        Dynamic window selection:\n",
    "          - If windows parameter not passed, window set chosen based on self.interval:\n",
    "                '1h':  [3, 6, 12, 24]\n",
    "                '12h': [2, 4, 6, 14]\n",
    "                '1d':  [3, 7, 14, 30]\n",
    "                '3d':  [2, 4, 7]\n",
    "                '1w':  [2, 4, 6]\n",
    "                '1M':  [3, 6, 12]\n",
    "          - If self.interval value absent from configuration, default windows [3, 5, 7] used.\n",
    "        \n",
    "        Lag configuration similar to previous implementation:\n",
    "                '1h':  [1, 12, 24, 168]\n",
    "                '12h': [1, 2, 3, 14]\n",
    "                '1d':  [1, 7, 14, 30]\n",
    "                '3d':  [1, 2, 7]\n",
    "                '1w':  [1, 2, 4]\n",
    "                '1M':  [1, 3, 6, 12]\n",
    "          - If self.interval not defined, lags from 1 to self.lag_features used.\n",
    "        \n",
    "        Usage example:\n",
    "             df_features = preprocessor.add_features(raw_df)\n",
    "        \n",
    "        :param df: DataFrame with time series, containing at minimum:\n",
    "                   - self.timestamp_column (e.g., 'Open_time')\n",
    "                   - self.target_column (e.g., 'Open')\n",
    "                   also desirable to have scaled data column, which can be named\n",
    "                   f'{self.target_column}_scaled'. If no such column, original series used.\n",
    "        :param windows: (optional) List of window sizes for rolling statistics calculation.\n",
    "        :param holiday_country: Country code for holiday determination (e.g., 'US').\n",
    "        :return: DataFrame with added features, indexed same as original DataFrame.\n",
    "        \"\"\"\n",
    "        # Create DataFrame copy and convert index to datetime\n",
    "        df = df.copy()\n",
    "\n",
    "        if not np.issubdtype(df.index.dtype, np.datetime64):\n",
    "            df.index = pd.to_datetime(df.index, errors='coerce')\n",
    "        \n",
    "        # Determine which column to base feature calculation on.\n",
    "        # If f'{self.target_column}_scaled' exists, use it, otherwise - original series.\n",
    "        base_col = f'{self.target_column}_scaled' if f'{self.target_column}_scaled' in df.columns else self.target_column\n",
    "        \n",
    "        features = df.copy()\n",
    "        features[base_col] = df[base_col]\n",
    "        \n",
    "        # --- Dynamic window selection ---\n",
    "        default_windows = {\n",
    "            '1h': [3, 6, 12, 24],\n",
    "            '12h': [2, 4, 6, 14],\n",
    "            '1d': [3, 7, 14, 30],\n",
    "            '3d': [2, 4, 7],\n",
    "            '1w': [2, 4, 6],\n",
    "            '1M': [3, 6, 12]\n",
    "        }\n",
    "        if windows is None:\n",
    "            windows = default_windows.get(self.interval, [3, 5, 7])\n",
    "        \n",
    "        # --- Lag features ---\n",
    "        lag_config = {\n",
    "            '1h': [1, 12, 24, 168],\n",
    "            '12h': [1, 2, 3, 14],\n",
    "            '1d': [1, 7, 14, 30],\n",
    "            '3d': [1, 2, 7],\n",
    "            '1w': [1, 2, 4],\n",
    "            '1M': [1, 3, 6, 12]\n",
    "        }\n",
    "        lags = lag_config.get(self.interval, list(range(1, self.lag_features + 1)))\n",
    "        for lag in lags:\n",
    "            features[f'lag_{lag}'] = df[base_col].shift(lag)\n",
    "        \n",
    "        # --- Rolling statistics (over multiple windows) ---\n",
    "        # For each given window, calculate set of statistics\n",
    "        for window in windows:\n",
    "            # Ensure window not larger than series length\n",
    "            if window > len(df):\n",
    "                continue\n",
    "            features[f'roll_mean_{window}'] = df[base_col].rolling(window=window, min_periods=1).mean()\n",
    "            features[f'roll_std_{window}'] = df[base_col].rolling(window=window, min_periods=1).std()\n",
    "            features[f'roll_min_{window}'] = df[base_col].rolling(window=window, min_periods=1).min()\n",
    "            features[f'roll_max_{window}'] = df[base_col].rolling(window=window, min_periods=1).max()\n",
    "            features[f'roll_median_{window}'] = df[base_col].rolling(window=window, min_periods=1).median()\n",
    "            if window > 2:\n",
    "                features[f'roll_skew_{window}'] = df[base_col].rolling(window=window, min_periods=1).skew()\n",
    "            if window > 3:\n",
    "                features[f'roll_kurtosis_{window}'] = df[base_col].rolling(window=window, min_periods=1).kurt() \n",
    "        \n",
    "        # --- Derivative features (based on original target_column) ---\n",
    "        # Can use original values here, as derivatives often reflect change dynamics\n",
    "        features['first_derivative'] = df[self.target_column].diff()\n",
    "        features['second_derivative'] = df[self.target_column].diff(2)\n",
    "        \n",
    "        # Rolling trend: calculate trend slope over rolling window for each window from windows\n",
    "        def calc_trend(x, win):\n",
    "            if len(x) < win:\n",
    "                return np.nan\n",
    "            # Linear regression: return slope\n",
    "            return np.polyfit(np.arange(len(x)), x, 1)[0]\n",
    "        \n",
    "        for window in windows:\n",
    "            # Apply function using .rolling().apply() method with raw=True\n",
    "            features[f'rolling_trend_{window}'] = df[self.target_column].rolling(window=window, min_periods=window).apply(lambda x: calc_trend(x, window), raw=True)\n",
    "        \n",
    "        # --- Financial indicators ---\n",
    "        # Returns (pct_change) and volatility (rolling std of returns)\n",
    "        features['returns'] = df[self.target_column].pct_change()\n",
    "        # For volatility, can use most suitable window (e.g., median from windows)\n",
    "        vol_window = windows[len(windows)//2] if windows else 7\n",
    "        features['volatility'] = features['returns'].rolling(window=vol_window, min_periods=1).std()\n",
    "\n",
    "        # --- Holidays ---\n",
    "        try:\n",
    "            dates_series = pd.Series(df.index, index=df.index)\n",
    "            years = dates_series.dt.year.unique().tolist()\n",
    "            country_holidays = holidays.CountryHoliday(holiday_country, years=years, expand=True, observed=True)\n",
    "            holiday_flags = dates_series.dt.date.map(lambda d: d in list(country_holidays.keys()))\n",
    "            features['is_holiday'] = pd.Series(holiday_flags.values, index=features.index)\n",
    "        except Exception as e:\n",
    "            print(f\"[LOG] Failed to load holidays for {holiday_country}: {e}\")\n",
    "            features['is_holiday'] = False\n",
    "        \n",
    "        # Remove rows with NaN resulting from shifts and window operations\n",
    "        features = features.dropna()\n",
    "        \n",
    "        return features\n",
    "\n",
    "\n",
    "    \n",
    "    def process(self, df):\n",
    "        \"\"\"\n",
    "        Unified time series preprocessing pipeline, returning X and y for ML.\n",
    "        \n",
    "        Steps:\n",
    "          1. Validate and convert timestamps.\n",
    "          2. Save original data.\n",
    "          3. Fill missing values for main and additional columns.\n",
    "          4. Analyze time series (log analysis results).\n",
    "          5. Remove outliers for main column.\n",
    "          6. Scale main column.\n",
    "          7. Generate features using unified method (lag and additional features).\n",
    "          8. Combine features with timestamp and scaled data.\n",
    "          9. Form final X matrix (features) and y vector (target variable).\n",
    "        \n",
    "        Example:\n",
    "            X, y = preprocessor.process(raw_df)\n",
    "        \n",
    "        :param df: Original DataFrame with time series, containing at minimum columns:\n",
    "                   self.timestamp_column and self.target_column.\n",
    "        :return: Tuple (X, y), where X is DataFrame with features, and y is Series with target variable.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df = df.copy()\n",
    "\n",
    "            # Convert timestamps\n",
    "            if self.timestamp_column not in df.columns:\n",
    "                raise ValueError(f\"Timestamp column '{self.timestamp_column}' missing in DataFrame.\")\n",
    "            if not np.issubdtype(df[self.timestamp_column].dtype, np.datetime64):\n",
    "                df[self.timestamp_column] = pd.to_datetime(df[self.timestamp_column], errors='coerce')\n",
    "        \n",
    "            # Save original data\n",
    "            self.original_data = df.copy()\n",
    "\n",
    "            # Set timestamp as index\n",
    "            df = df.set_index(self.timestamp_column)\n",
    "            \n",
    "            # Fill missing values for main and additional columns\n",
    "            cols_to_process = [self.target_column] + self.additional_columns\n",
    "            for col in cols_to_process:\n",
    "                df[col] = self.fill_missing_values(df[col])\n",
    "        \n",
    "            # Analyze main series (logging for diagnostics)\n",
    "            analysis = self.analyze_series(df[self.target_column])\n",
    "            print(\"[LOG] Time series analysis:\", analysis)\n",
    "        \n",
    "            # Remove outliers for main column\n",
    "            df[f'{self.target_column}_removed_outliers'] = self.remove_outliers(df[self.target_column])\n",
    "        \n",
    "            # Scale main column\n",
    "            scaled_series = self.scale_data(df[f'{self.target_column}_removed_outliers'])\n",
    "            df[f'{self.target_column}_scaled'] = scaled_series\n",
    "\n",
    "            # Generate features: lag and additional features\n",
    "            df = self.add_features(df)\n",
    "\n",
    "            # Sort by timestamp\n",
    "            df.sort_index(inplace=True)\n",
    "            print(\"[LOG] Pipeline completed. Final observation count:\", len(df))\n",
    "\n",
    "            # Form X and y:\n",
    "            # y - target variable (scaled value)\n",
    "            # X - feature matrix, excluding timestamp and target variable\n",
    "            y = df[f'{self.target_column}_scaled']\n",
    "            X = df.drop(columns=[f'{self.target_column}_scaled'])\n",
    "\n",
    "            return X, y\n",
    "    \n",
    "        except Exception as e:\n",
    "            raise Exception(f'[ml_manager/TimeSeriesPreprocessorManager] error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cd507a9-7048-4df3-b41e-204249e01f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open_time</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Close_time</th>\n",
       "      <th>Quote_asset_volume</th>\n",
       "      <th>Number_of_trades</th>\n",
       "      <th>Taker_buy_base_asset_volume</th>\n",
       "      <th>Taker_buy_quote_asset_volume</th>\n",
       "      <th>ts_id</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-08-17 00:00:00</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4485.39</td>\n",
       "      <td>4261.32</td>\n",
       "      <td>4427.30</td>\n",
       "      <td>145.708747</td>\n",
       "      <td>2017-08-17 11:59:59</td>\n",
       "      <td>6.356955e+05</td>\n",
       "      <td>582</td>\n",
       "      <td>122.801360</td>\n",
       "      <td>5.367015e+05</td>\n",
       "      <td>btcusdt_12h</td>\n",
       "      <td>binance_api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-08-17 12:00:00</td>\n",
       "      <td>4436.06</td>\n",
       "      <td>4485.39</td>\n",
       "      <td>4200.74</td>\n",
       "      <td>4285.08</td>\n",
       "      <td>649.441630</td>\n",
       "      <td>2017-08-17 23:59:59</td>\n",
       "      <td>2.819075e+06</td>\n",
       "      <td>2845</td>\n",
       "      <td>493.447181</td>\n",
       "      <td>2.141515e+06</td>\n",
       "      <td>btcusdt_12h</td>\n",
       "      <td>binance_api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-08-18 00:00:00</td>\n",
       "      <td>4285.08</td>\n",
       "      <td>4371.52</td>\n",
       "      <td>4134.61</td>\n",
       "      <td>4340.31</td>\n",
       "      <td>720.722201</td>\n",
       "      <td>2017-08-18 11:59:59</td>\n",
       "      <td>3.085356e+06</td>\n",
       "      <td>3051</td>\n",
       "      <td>585.408001</td>\n",
       "      <td>2.508148e+06</td>\n",
       "      <td>btcusdt_12h</td>\n",
       "      <td>binance_api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-08-18 12:00:00</td>\n",
       "      <td>4320.52</td>\n",
       "      <td>4340.31</td>\n",
       "      <td>3938.77</td>\n",
       "      <td>4108.37</td>\n",
       "      <td>479.166063</td>\n",
       "      <td>2017-08-18 23:59:59</td>\n",
       "      <td>2.001602e+06</td>\n",
       "      <td>2182</td>\n",
       "      <td>387.460709</td>\n",
       "      <td>1.620975e+06</td>\n",
       "      <td>btcusdt_12h</td>\n",
       "      <td>binance_api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-08-19 00:00:00</td>\n",
       "      <td>4108.37</td>\n",
       "      <td>4184.69</td>\n",
       "      <td>3850.00</td>\n",
       "      <td>3957.60</td>\n",
       "      <td>298.518569</td>\n",
       "      <td>2017-08-19 11:59:59</td>\n",
       "      <td>1.214383e+06</td>\n",
       "      <td>1559</td>\n",
       "      <td>213.297785</td>\n",
       "      <td>8.706680e+05</td>\n",
       "      <td>btcusdt_12h</td>\n",
       "      <td>binance_api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6149</th>\n",
       "      <td>2026-01-16 12:00:00</td>\n",
       "      <td>95429.09</td>\n",
       "      <td>95809.08</td>\n",
       "      <td>94293.46</td>\n",
       "      <td>95550.94</td>\n",
       "      <td>6051.259530</td>\n",
       "      <td>2026-01-16 23:59:59</td>\n",
       "      <td>5.753072e+08</td>\n",
       "      <td>1962043</td>\n",
       "      <td>2867.231240</td>\n",
       "      <td>2.726688e+08</td>\n",
       "      <td>btcusdt_12h</td>\n",
       "      <td>binance_api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6150</th>\n",
       "      <td>2026-01-17 00:00:00</td>\n",
       "      <td>95550.94</td>\n",
       "      <td>95578.20</td>\n",
       "      <td>95021.67</td>\n",
       "      <td>95323.86</td>\n",
       "      <td>1694.352380</td>\n",
       "      <td>2026-01-17 11:59:59</td>\n",
       "      <td>1.614328e+08</td>\n",
       "      <td>459123</td>\n",
       "      <td>839.789580</td>\n",
       "      <td>8.001233e+07</td>\n",
       "      <td>btcusdt_12h</td>\n",
       "      <td>binance_api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6151</th>\n",
       "      <td>2026-01-17 12:00:00</td>\n",
       "      <td>95323.85</td>\n",
       "      <td>95639.45</td>\n",
       "      <td>95050.00</td>\n",
       "      <td>95147.77</td>\n",
       "      <td>2586.448990</td>\n",
       "      <td>2026-01-17 23:59:59</td>\n",
       "      <td>2.467292e+08</td>\n",
       "      <td>561404</td>\n",
       "      <td>1220.063250</td>\n",
       "      <td>1.163867e+08</td>\n",
       "      <td>btcusdt_12h</td>\n",
       "      <td>binance_api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6152</th>\n",
       "      <td>2026-01-18 00:00:00</td>\n",
       "      <td>95147.77</td>\n",
       "      <td>95295.39</td>\n",
       "      <td>94876.33</td>\n",
       "      <td>95168.06</td>\n",
       "      <td>2386.333180</td>\n",
       "      <td>2026-01-18 11:59:59</td>\n",
       "      <td>2.269771e+08</td>\n",
       "      <td>513805</td>\n",
       "      <td>1112.374100</td>\n",
       "      <td>1.058134e+08</td>\n",
       "      <td>btcusdt_12h</td>\n",
       "      <td>binance_api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6153</th>\n",
       "      <td>2026-01-18 12:00:00</td>\n",
       "      <td>95168.06</td>\n",
       "      <td>95376.24</td>\n",
       "      <td>94910.00</td>\n",
       "      <td>95183.55</td>\n",
       "      <td>1256.213490</td>\n",
       "      <td>2026-01-18 23:59:59</td>\n",
       "      <td>1.194810e+08</td>\n",
       "      <td>279502</td>\n",
       "      <td>566.668850</td>\n",
       "      <td>5.389214e+07</td>\n",
       "      <td>btcusdt_12h</td>\n",
       "      <td>binance_api</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6154 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Open_time      Open      High       Low     Close       Volume  \\\n",
       "0    2017-08-17 00:00:00   4261.48   4485.39   4261.32   4427.30   145.708747   \n",
       "1    2017-08-17 12:00:00   4436.06   4485.39   4200.74   4285.08   649.441630   \n",
       "2    2017-08-18 00:00:00   4285.08   4371.52   4134.61   4340.31   720.722201   \n",
       "3    2017-08-18 12:00:00   4320.52   4340.31   3938.77   4108.37   479.166063   \n",
       "4    2017-08-19 00:00:00   4108.37   4184.69   3850.00   3957.60   298.518569   \n",
       "...                  ...       ...       ...       ...       ...          ...   \n",
       "6149 2026-01-16 12:00:00  95429.09  95809.08  94293.46  95550.94  6051.259530   \n",
       "6150 2026-01-17 00:00:00  95550.94  95578.20  95021.67  95323.86  1694.352380   \n",
       "6151 2026-01-17 12:00:00  95323.85  95639.45  95050.00  95147.77  2586.448990   \n",
       "6152 2026-01-18 00:00:00  95147.77  95295.39  94876.33  95168.06  2386.333180   \n",
       "6153 2026-01-18 12:00:00  95168.06  95376.24  94910.00  95183.55  1256.213490   \n",
       "\n",
       "              Close_time  Quote_asset_volume  Number_of_trades  \\\n",
       "0    2017-08-17 11:59:59        6.356955e+05               582   \n",
       "1    2017-08-17 23:59:59        2.819075e+06              2845   \n",
       "2    2017-08-18 11:59:59        3.085356e+06              3051   \n",
       "3    2017-08-18 23:59:59        2.001602e+06              2182   \n",
       "4    2017-08-19 11:59:59        1.214383e+06              1559   \n",
       "...                  ...                 ...               ...   \n",
       "6149 2026-01-16 23:59:59        5.753072e+08           1962043   \n",
       "6150 2026-01-17 11:59:59        1.614328e+08            459123   \n",
       "6151 2026-01-17 23:59:59        2.467292e+08            561404   \n",
       "6152 2026-01-18 11:59:59        2.269771e+08            513805   \n",
       "6153 2026-01-18 23:59:59        1.194810e+08            279502   \n",
       "\n",
       "      Taker_buy_base_asset_volume  Taker_buy_quote_asset_volume        ts_id  \\\n",
       "0                      122.801360                  5.367015e+05  btcusdt_12h   \n",
       "1                      493.447181                  2.141515e+06  btcusdt_12h   \n",
       "2                      585.408001                  2.508148e+06  btcusdt_12h   \n",
       "3                      387.460709                  1.620975e+06  btcusdt_12h   \n",
       "4                      213.297785                  8.706680e+05  btcusdt_12h   \n",
       "...                           ...                           ...          ...   \n",
       "6149                  2867.231240                  2.726688e+08  btcusdt_12h   \n",
       "6150                   839.789580                  8.001233e+07  btcusdt_12h   \n",
       "6151                  1220.063250                  1.163867e+08  btcusdt_12h   \n",
       "6152                  1112.374100                  1.058134e+08  btcusdt_12h   \n",
       "6153                   566.668850                  5.389214e+07  btcusdt_12h   \n",
       "\n",
       "           Source  \n",
       "0     binance_api  \n",
       "1     binance_api  \n",
       "2     binance_api  \n",
       "3     binance_api  \n",
       "4     binance_api  \n",
       "...           ...  \n",
       "6149  binance_api  \n",
       "6150  binance_api  \n",
       "6151  binance_api  \n",
       "6152  binance_api  \n",
       "6153  binance_api  \n",
       "\n",
       "[6154 rows x 13 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30f7e861-e916-4f60-a0fc-0f2e629cd4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gb/tqzzyhfj48zf1dmpwjhzh_g40000gn/T/ipykernel_38408/3830646174.py:83: InterpolationWarning: The test statistic is outside of the range of p-values available in the\n",
      "look-up table. The actual p-value is smaller than the p-value returned.\n",
      "\n",
      "  kpss_result = kpss(data.dropna(), regression='c', nlags=\"auto\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] Time series analysis: {'missing_values': 0, 'zscore_outliers': 0, 'iqr_outliers': 0, 'mad_outliers': 12, 'adf_pvalue': 0.8824802235147571, 'kpss_pvalue': 0.01, 'rolling_mean_cv': 0.8749471392789483, 'rolling_std_cv': 0.7250906003473109, 'lag1_autocorrelation': 0.9995792490385362, 'qq_correlation': 0.9253705686223104, 'stationary': False}\n",
      "[LOG] Applying ACF!!! 1\n",
      "[LOG] Ignoring FFT candidate: 3077 (exceeds threshold)\n",
      "[LOG] Applying Fallback!!!\n",
      "[LOG] Determined period: 14, candidates: [1]\n",
      "[LOG] STL method selected\n",
      "[LOG] Initial data stats: {'min': 2870.9, 'max': 120529.35, 'mean': 36166.72933009934, 'std': 32165.42770707314, 'median': 26438.364999999998, 'IQR': 47448.63407255327}\n",
      "[LOG] Scaled data stats: {'min': -0.4966942770989614, 'max': 1.983007242234336, 'mean': 0.20502938641444945, 'std': 0.6778448917212686}\n",
      "[LOG] Pipeline completed. Final observation count: 6140\n"
     ]
    }
   ],
   "source": [
    "# Create instance of TimeSeriesPreprocessorManager class\n",
    "ts_preprocessor_manager = TimeSeriesPreprocessorManager('12h')\n",
    "X, y = ts_preprocessor_manager.process(ts_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f68904e1-0e8e-4148-b1df-5473bd9dcd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gb/tqzzyhfj48zf1dmpwjhzh_g40000gn/T/ipykernel_38408/3830646174.py:83: InterpolationWarning: The test statistic is outside of the range of p-values available in the\n",
      "look-up table. The actual p-value is smaller than the p-value returned.\n",
      "\n",
      "  kpss_result = kpss(data.dropna(), regression='c', nlags=\"auto\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] Time series analysis: {'missing_values': 0, 'zscore_outliers': 0, 'iqr_outliers': 0, 'mad_outliers': 12, 'adf_pvalue': 0.8824802235147571, 'kpss_pvalue': 0.01, 'rolling_mean_cv': 0.8749471392789483, 'rolling_std_cv': 0.7250906003473109, 'lag1_autocorrelation': 0.9995792490385362, 'qq_correlation': 0.9253705686223104, 'stationary': False}\n",
      "[LOG] Applying ACF!!! 1\n",
      "[LOG] Ignoring FFT candidate: 3077 (exceeds threshold)\n",
      "[LOG] Applying Fallback!!!\n",
      "[LOG] Determined period: 14, candidates: [1]\n",
      "[LOG] STL method selected\n",
      "[LOG] Initial data stats: {'min': 2870.9, 'max': 120529.35, 'mean': 36161.18329604656, 'std': 32166.638938058924, 'median': 26438.364999999998, 'IQR': 47448.554113987026}\n",
      "[LOG] Scaled data stats: {'min': -0.49669511410997264, 'max': 1.9830105839255405, 'mean': 0.20491284671581697, 'std': 0.6778715591740715}\n",
      "[LOG] Pipeline completed. Final observation count: 6140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gb/tqzzyhfj48zf1dmpwjhzh_g40000gn/T/ipykernel_38408/3830646174.py:83: InterpolationWarning: The test statistic is outside of the range of p-values available in the\n",
      "look-up table. The actual p-value is smaller than the p-value returned.\n",
      "\n",
      "  kpss_result = kpss(data.dropna(), regression='c', nlags=\"auto\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] Time series analysis: {'missing_values': 0, 'zscore_outliers': 0, 'iqr_outliers': 0, 'mad_outliers': 12, 'adf_pvalue': 0.8824802235147571, 'kpss_pvalue': 0.01, 'rolling_mean_cv': 0.8749471392789483, 'rolling_std_cv': 0.7250906003473109, 'lag1_autocorrelation': 0.9995792490385362, 'qq_correlation': 0.9253705686223104, 'stationary': False}\n",
      "[LOG] Applying ACF!!! 1\n",
      "[LOG] Ignoring FFT candidate: 3077 (exceeds threshold)\n",
      "[LOG] Applying Fallback!!!\n",
      "[LOG] Determined period: 14, candidates: [1]\n",
      "[LOG] STL method selected\n",
      "[LOG] Initial data stats: {'min': 2870.9, 'max': 120529.35, 'mean': 36172.19170485849, 'std': 32170.294907824817, 'median': 26438.364999999998, 'IQR': 47448.68384067977}\n",
      "[LOG] Scaled data stats: {'min': -0.49669375612468747, 'max': 1.9830051622913896, 'mean': 0.20514429309656151, 'std': 0.6779467506101546}\n",
      "[LOG] Pipeline completed. Final observation count: 6140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gb/tqzzyhfj48zf1dmpwjhzh_g40000gn/T/ipykernel_38408/3830646174.py:83: InterpolationWarning: The test statistic is outside of the range of p-values available in the\n",
      "look-up table. The actual p-value is smaller than the p-value returned.\n",
      "\n",
      "  kpss_result = kpss(data.dropna(), regression='c', nlags=\"auto\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] Time series analysis: {'missing_values': 0, 'zscore_outliers': 0, 'iqr_outliers': 0, 'mad_outliers': 12, 'adf_pvalue': 0.8824802235147571, 'kpss_pvalue': 0.01, 'rolling_mean_cv': 0.8749471392789483, 'rolling_std_cv': 0.7250906003473109, 'lag1_autocorrelation': 0.9995792490385362, 'qq_correlation': 0.9253705686223104, 'stationary': False}\n",
      "[LOG] Applying ACF!!! 1\n",
      "[LOG] Ignoring FFT candidate: 3077 (exceeds threshold)\n",
      "[LOG] Applying Fallback!!!\n",
      "[LOG] Determined period: 14, candidates: [1]\n",
      "[LOG] STL method selected\n",
      "[LOG] Initial data stats: {'min': 2870.9, 'max': 120529.35, 'mean': 36192.73776921112, 'std': 32184.800477618726, 'median': 26438.364999999998, 'IQR': 47465.73}\n",
      "[LOG] Scaled data stats: {'min': -0.4965153806757, 'max': 1.9822930143495108, 'mean': 0.20550348154786868, 'std': 0.6780088586379134}\n",
      "[LOG] Pipeline completed. Final observation count: 6140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gb/tqzzyhfj48zf1dmpwjhzh_g40000gn/T/ipykernel_38408/3830646174.py:83: InterpolationWarning: The test statistic is outside of the range of p-values available in the\n",
      "look-up table. The actual p-value is smaller than the p-value returned.\n",
      "\n",
      "  kpss_result = kpss(data.dropna(), regression='c', nlags=\"auto\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] Time series analysis: {'missing_values': 0, 'zscore_outliers': 0, 'iqr_outliers': 0, 'mad_outliers': 12, 'adf_pvalue': 0.8824802235147571, 'kpss_pvalue': 0.01, 'rolling_mean_cv': 0.8749471392789483, 'rolling_std_cv': 0.7250906003473109, 'lag1_autocorrelation': 0.9995792490385362, 'qq_correlation': 0.9253705686223104, 'stationary': False}\n",
      "[LOG] Applying ACF!!! 1\n",
      "[LOG] Ignoring FFT candidate: 3077 (exceeds threshold)\n",
      "[LOG] Applying Fallback!!!\n",
      "[LOG] Determined period: 14, candidates: [1]\n",
      "[LOG] STL method selected\n",
      "[LOG] Initial data stats: {'min': 2870.9, 'max': 120529.35, 'mean': 36184.16925869915, 'std': 32179.810767214763, 'median': 26438.364999999998, 'IQR': 47465.0775}\n",
      "[LOG] Scaled data stats: {'min': -0.49652220624731935, 'max': 1.982320264830496, 'mean': 0.20532578417678035, 'std': 0.6779130639096019}\n",
      "[LOG] Pipeline completed. Final observation count: 6140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gb/tqzzyhfj48zf1dmpwjhzh_g40000gn/T/ipykernel_38408/3830646174.py:83: InterpolationWarning: The test statistic is outside of the range of p-values available in the\n",
      "look-up table. The actual p-value is smaller than the p-value returned.\n",
      "\n",
      "  kpss_result = kpss(data.dropna(), regression='c', nlags=\"auto\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] Time series analysis: {'missing_values': 0, 'zscore_outliers': 0, 'iqr_outliers': 0, 'mad_outliers': 12, 'adf_pvalue': 0.8824802235147571, 'kpss_pvalue': 0.01, 'rolling_mean_cv': 0.8749471392789483, 'rolling_std_cv': 0.7250906003473109, 'lag1_autocorrelation': 0.9995792490385362, 'qq_correlation': 0.9253705686223104, 'stationary': False}\n",
      "[LOG] Applying ACF!!! 1\n",
      "[LOG] Ignoring FFT candidate: 3077 (exceeds threshold)\n",
      "[LOG] Applying Fallback!!!\n",
      "[LOG] Determined period: 14, candidates: [1]\n",
      "[LOG] STL method selected\n",
      "[LOG] Initial data stats: {'min': 2870.9, 'max': 120529.35, 'mean': 36151.358757893155, 'std': 32160.49736529105, 'median': 26438.364999999998, 'IQR': 47417.635}\n",
      "[LOG] Scaled data stats: {'min': -0.497018988821353, 'max': 1.984303624590303, 'mean': 0.20483927040842836, 'std': 0.6781840613528363}\n",
      "[LOG] Pipeline completed. Final observation count: 6140\n",
      "[LOG] Time series analysis: {'missing_values': 0, 'zscore_outliers': 0, 'iqr_outliers': 0, 'mad_outliers': 12, 'adf_pvalue': 0.8824802235147571, 'kpss_pvalue': 0.01, 'rolling_mean_cv': 0.8749471392789483, 'rolling_std_cv': 0.7250906003473109, 'lag1_autocorrelation': 0.9995792490385362, 'qq_correlation': 0.9253705686223104, 'stationary': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gb/tqzzyhfj48zf1dmpwjhzh_g40000gn/T/ipykernel_38408/3830646174.py:83: InterpolationWarning: The test statistic is outside of the range of p-values available in the\n",
      "look-up table. The actual p-value is smaller than the p-value returned.\n",
      "\n",
      "  kpss_result = kpss(data.dropna(), regression='c', nlags=\"auto\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] Applying ACF!!! 1\n",
      "[LOG] Ignoring FFT candidate: 3077 (exceeds threshold)\n",
      "[LOG] Applying Fallback!!!\n",
      "[LOG] Determined period: 14, candidates: [1]\n",
      "[LOG] STL method selected\n",
      "[LOG] Initial data stats: {'min': 2870.9, 'max': 120529.35, 'mean': 36166.536532327395, 'std': 32167.756621790166, 'median': 26438.364999999998, 'IQR': 47448.63407255327}\n",
      "[LOG] Scaled data stats: {'min': -0.4966942770989614, 'max': 1.983007242234336, 'mean': 0.20502532311998994, 'std': 0.6778939705943541}\n",
      "[LOG] Pipeline completed. Final observation count: 6140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gb/tqzzyhfj48zf1dmpwjhzh_g40000gn/T/ipykernel_38408/3830646174.py:83: InterpolationWarning: The test statistic is outside of the range of p-values available in the\n",
      "look-up table. The actual p-value is smaller than the p-value returned.\n",
      "\n",
      "  kpss_result = kpss(data.dropna(), regression='c', nlags=\"auto\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] Time series analysis: {'missing_values': 0, 'zscore_outliers': 0, 'iqr_outliers': 0, 'mad_outliers': 12, 'adf_pvalue': 0.8824802235147571, 'kpss_pvalue': 0.01, 'rolling_mean_cv': 0.8749471392789483, 'rolling_std_cv': 0.7250906003473109, 'lag1_autocorrelation': 0.9995792490385362, 'qq_correlation': 0.9253705686223104, 'stationary': False}\n",
      "[LOG] Applying ACF!!! 1\n",
      "[LOG] Ignoring FFT candidate: 3077 (exceeds threshold)\n",
      "[LOG] Applying Fallback!!!\n",
      "[LOG] Determined period: 14, candidates: [1]\n",
      "[LOG] STL method selected\n",
      "[LOG] Initial data stats: {'min': 2870.9, 'max': 120529.35, 'mean': 36125.655142783864, 'std': 32139.228135638587, 'median': 26405.88683208988, 'IQR': 47337.44823866553}\n",
      "[LOG] Scaled data stats: {'min': -0.4971748099608452, 'max': 1.9883510132054285, 'mean': 0.2053293676010785, 'std': 0.678883589595276}\n",
      "[LOG] Pipeline completed. Final observation count: 6140\n",
      "[LOG] Time series analysis: {'missing_values': 0, 'zscore_outliers': 0, 'iqr_outliers': 0, 'mad_outliers': 12, 'adf_pvalue': 0.8824802235147571, 'kpss_pvalue': 0.01, 'rolling_mean_cv': 0.8749471392789483, 'rolling_std_cv': 0.7250906003473109, 'lag1_autocorrelation': 0.9995792490385362, 'qq_correlation': 0.9253705686223104, 'stationary': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gb/tqzzyhfj48zf1dmpwjhzh_g40000gn/T/ipykernel_38408/3830646174.py:83: InterpolationWarning: The test statistic is outside of the range of p-values available in the\n",
      "look-up table. The actual p-value is smaller than the p-value returned.\n",
      "\n",
      "  kpss_result = kpss(data.dropna(), regression='c', nlags=\"auto\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] Applying ACF!!! 1\n",
      "[LOG] Ignoring FFT candidate: 3077 (exceeds threshold)\n",
      "[LOG] Applying Fallback!!!\n",
      "[LOG] Determined period: 14, candidates: [1]\n",
      "[LOG] STL method selected\n",
      "[LOG] Initial data stats: {'min': 2870.9, 'max': 120529.35, 'mean': 36172.843801324394, 'std': 32175.48632129042, 'median': 26438.364999999998, 'IQR': 47465.73}\n",
      "[LOG] Scaled data stats: {'min': -0.4965153806757, 'max': 1.9822930143495108, 'mean': 0.20508435878526254, 'std': 0.6778126455060127}\n",
      "[LOG] Pipeline completed. Final observation count: 6140\n",
      "2.19 s ± 635 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Error evaluating contamination 0.51: The 'contamination' parameter of IsolationForest must be a str among {'auto'} or a float in the range (0.0, 0.5]. Got 0.51 instead.\n",
      "Error evaluating contamination 0.51: The 'contamination' parameter of IsolationForest must be a str among {'auto'} or a float in the range (0.0, 0.5]. Got 0.51 instead.\n",
      "Error evaluating contamination 0.51: The 'contamination' parameter of IsolationForest must be a str among {'auto'} or a float in the range (0.0, 0.5]. Got 0.51 instead.\n",
      "Error evaluating contamination 0.51: The 'contamination' parameter of IsolationForest must be a str among {'auto'} or a float in the range (0.0, 0.5]. Got 0.51 instead.\n",
      "Error evaluating contamination 0.51: The 'contamination' parameter of IsolationForest must be a str among {'auto'} or a float in the range (0.0, 0.5]. Got 0.51 instead.\n",
      "Error evaluating contamination 0.51: The 'contamination' parameter of IsolationForest must be a str among {'auto'} or a float in the range (0.0, 0.5]. Got 0.51 instead.\n",
      "Error evaluating contamination 0.51: The 'contamination' parameter of IsolationForest must be a str among {'auto'} or a float in the range (0.0, 0.5]. Got 0.51 instead.\n",
      "Error evaluating contamination 0.51: The 'contamination' parameter of IsolationForest must be a str among {'auto'} or a float in the range (0.0, 0.5]. Got 0.51 instead.\n",
      "Error evaluating contamination 0.51: The 'contamination' parameter of IsolationForest must be a str among {'auto'} or a float in the range (0.0, 0.5]. Got 0.51 instead.\n"
     ]
    }
   ],
   "source": [
    "%timeit ts_preprocessor_manager.process(ts_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d50d873-f0db-488c-9f21-d68654d12877",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
