{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a299860-960e-4c94-bfac-78980c0e8b93",
   "metadata": {},
   "source": [
    "###  v1.1. optimized version but still in single class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "290f65d1-88fa-4e22-877f-daedea64e4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14c2a43b-d52c-4c74-8d8c-d1db830d077c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "# для TimeSeriesPreprocessorManager\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import statsmodels.api as sm\n",
    "from prophet import Prophet\n",
    "from statsmodels.tsa.seasonal import STL, seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf, kpss\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from scipy.stats import zscore\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import pmdarima as pm  # INSTALL TO REQ\n",
    "from scipy import signal\n",
    "from scipy.stats import zscore, probplot, pearsonr\n",
    "from joblib import Parallel, delayed\n",
    "from tbats import TBATS\n",
    "import holidays\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from tbats import TBATS\n",
    "from prophet import Prophet\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.early_stop import no_progress_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b401f9-861a-4b19-aad5-bd6e5ff37147",
   "metadata": {},
   "source": [
    "# Получение входных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83060613-f25e-4b5c-b041-cebe1933eee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLICKHOUSE_HOST='localhost'\n",
    "CLICKHOUSE_PORT=8123\n",
    "CLICKHOUSE_USER='CLICKHOUSE_USER' # INPUT YOUR CREDENTIAL \n",
    "CLICKHOUSE_USER='CLICKHOUSE_USER' \n",
    "CLICKHOUSE_PASSWORD='CLICKHOUSE_PASSWORD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e43dfd8-5276-49fc-b660-90264da65355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# receiving data\n",
    "\n",
    "import clickhouse_connect\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "client = clickhouse_connect.get_client(\n",
    "                host=CLICKHOUSE_HOST,\n",
    "                port=CLICKHOUSE_PORT,\n",
    "                user=CLICKHOUSE_USER,\n",
    "                password=CLICKHOUSE_PASSWORD\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a929d762-3f7b-4fbf-9bfa-a046e18420fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load time series from db\n",
    "tablename = \"CRYPTO.btcusdt_12h_raw\"\n",
    "ch_response = client.query(f\"SELECT * FROM {tablename} ORDER BY Open_time\")\n",
    "ts_data = pd.DataFrame(ch_response.result_rows, columns=ch_response.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "911ada47-c338-4b8b-8e57-9852820527f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open_time</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Close_time</th>\n",
       "      <th>Quote_asset_volume</th>\n",
       "      <th>Number_of_trades</th>\n",
       "      <th>Taker_buy_base_asset_volume</th>\n",
       "      <th>Taker_buy_quote_asset_volume</th>\n",
       "      <th>ts_id</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-08-17 00:00:00</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4485.39</td>\n",
       "      <td>4261.32</td>\n",
       "      <td>4427.30</td>\n",
       "      <td>145.708747</td>\n",
       "      <td>2017-08-17 11:59:59</td>\n",
       "      <td>6.356955e+05</td>\n",
       "      <td>582</td>\n",
       "      <td>122.801360</td>\n",
       "      <td>5.367015e+05</td>\n",
       "      <td>btcusdt_12h</td>\n",
       "      <td>binance_api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-08-17 12:00:00</td>\n",
       "      <td>4436.06</td>\n",
       "      <td>4485.39</td>\n",
       "      <td>4200.74</td>\n",
       "      <td>4285.08</td>\n",
       "      <td>649.441630</td>\n",
       "      <td>2017-08-17 23:59:59</td>\n",
       "      <td>2.819075e+06</td>\n",
       "      <td>2845</td>\n",
       "      <td>493.447181</td>\n",
       "      <td>2.141515e+06</td>\n",
       "      <td>btcusdt_12h</td>\n",
       "      <td>binance_api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-08-18 00:00:00</td>\n",
       "      <td>4285.08</td>\n",
       "      <td>4371.52</td>\n",
       "      <td>4134.61</td>\n",
       "      <td>4340.31</td>\n",
       "      <td>720.722201</td>\n",
       "      <td>2017-08-18 11:59:59</td>\n",
       "      <td>3.085356e+06</td>\n",
       "      <td>3051</td>\n",
       "      <td>585.408001</td>\n",
       "      <td>2.508148e+06</td>\n",
       "      <td>btcusdt_12h</td>\n",
       "      <td>binance_api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-08-18 12:00:00</td>\n",
       "      <td>4320.52</td>\n",
       "      <td>4340.31</td>\n",
       "      <td>3938.77</td>\n",
       "      <td>4108.37</td>\n",
       "      <td>479.166063</td>\n",
       "      <td>2017-08-18 23:59:59</td>\n",
       "      <td>2.001602e+06</td>\n",
       "      <td>2182</td>\n",
       "      <td>387.460709</td>\n",
       "      <td>1.620975e+06</td>\n",
       "      <td>btcusdt_12h</td>\n",
       "      <td>binance_api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-08-19 00:00:00</td>\n",
       "      <td>4108.37</td>\n",
       "      <td>4184.69</td>\n",
       "      <td>3850.00</td>\n",
       "      <td>3957.60</td>\n",
       "      <td>298.518569</td>\n",
       "      <td>2017-08-19 11:59:59</td>\n",
       "      <td>1.214383e+06</td>\n",
       "      <td>1559</td>\n",
       "      <td>213.297785</td>\n",
       "      <td>8.706680e+05</td>\n",
       "      <td>btcusdt_12h</td>\n",
       "      <td>binance_api</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Open_time     Open     High      Low    Close      Volume  \\\n",
       "0 2017-08-17 00:00:00  4261.48  4485.39  4261.32  4427.30  145.708747   \n",
       "1 2017-08-17 12:00:00  4436.06  4485.39  4200.74  4285.08  649.441630   \n",
       "2 2017-08-18 00:00:00  4285.08  4371.52  4134.61  4340.31  720.722201   \n",
       "3 2017-08-18 12:00:00  4320.52  4340.31  3938.77  4108.37  479.166063   \n",
       "4 2017-08-19 00:00:00  4108.37  4184.69  3850.00  3957.60  298.518569   \n",
       "\n",
       "           Close_time  Quote_asset_volume  Number_of_trades  \\\n",
       "0 2017-08-17 11:59:59        6.356955e+05               582   \n",
       "1 2017-08-17 23:59:59        2.819075e+06              2845   \n",
       "2 2017-08-18 11:59:59        3.085356e+06              3051   \n",
       "3 2017-08-18 23:59:59        2.001602e+06              2182   \n",
       "4 2017-08-19 11:59:59        1.214383e+06              1559   \n",
       "\n",
       "   Taker_buy_base_asset_volume  Taker_buy_quote_asset_volume        ts_id  \\\n",
       "0                   122.801360                  5.367015e+05  btcusdt_12h   \n",
       "1                   493.447181                  2.141515e+06  btcusdt_12h   \n",
       "2                   585.408001                  2.508148e+06  btcusdt_12h   \n",
       "3                   387.460709                  1.620975e+06  btcusdt_12h   \n",
       "4                   213.297785                  8.706680e+05  btcusdt_12h   \n",
       "\n",
       "        Source  \n",
       "0  binance_api  \n",
       "1  binance_api  \n",
       "2  binance_api  \n",
       "3  binance_api  \n",
       "4  binance_api  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b07e126-a2a5-499c-81df-362ce5ae956a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open_time</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Close_time</th>\n",
       "      <th>Quote_asset_volume</th>\n",
       "      <th>Number_of_trades</th>\n",
       "      <th>Taker_buy_base_asset_volume</th>\n",
       "      <th>Taker_buy_quote_asset_volume</th>\n",
       "      <th>ts_id</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6149</th>\n",
       "      <td>2026-01-16 12:00:00</td>\n",
       "      <td>95429.09</td>\n",
       "      <td>95809.08</td>\n",
       "      <td>94293.46</td>\n",
       "      <td>95550.94</td>\n",
       "      <td>6051.25953</td>\n",
       "      <td>2026-01-16 23:59:59</td>\n",
       "      <td>5.753072e+08</td>\n",
       "      <td>1962043</td>\n",
       "      <td>2867.23124</td>\n",
       "      <td>2.726688e+08</td>\n",
       "      <td>btcusdt_12h</td>\n",
       "      <td>binance_api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6150</th>\n",
       "      <td>2026-01-17 00:00:00</td>\n",
       "      <td>95550.94</td>\n",
       "      <td>95578.20</td>\n",
       "      <td>95021.67</td>\n",
       "      <td>95323.86</td>\n",
       "      <td>1694.35238</td>\n",
       "      <td>2026-01-17 11:59:59</td>\n",
       "      <td>1.614328e+08</td>\n",
       "      <td>459123</td>\n",
       "      <td>839.78958</td>\n",
       "      <td>8.001233e+07</td>\n",
       "      <td>btcusdt_12h</td>\n",
       "      <td>binance_api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6151</th>\n",
       "      <td>2026-01-17 12:00:00</td>\n",
       "      <td>95323.85</td>\n",
       "      <td>95639.45</td>\n",
       "      <td>95050.00</td>\n",
       "      <td>95147.77</td>\n",
       "      <td>2586.44899</td>\n",
       "      <td>2026-01-17 23:59:59</td>\n",
       "      <td>2.467292e+08</td>\n",
       "      <td>561404</td>\n",
       "      <td>1220.06325</td>\n",
       "      <td>1.163867e+08</td>\n",
       "      <td>btcusdt_12h</td>\n",
       "      <td>binance_api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6152</th>\n",
       "      <td>2026-01-18 00:00:00</td>\n",
       "      <td>95147.77</td>\n",
       "      <td>95295.39</td>\n",
       "      <td>94876.33</td>\n",
       "      <td>95168.06</td>\n",
       "      <td>2386.33318</td>\n",
       "      <td>2026-01-18 11:59:59</td>\n",
       "      <td>2.269771e+08</td>\n",
       "      <td>513805</td>\n",
       "      <td>1112.37410</td>\n",
       "      <td>1.058134e+08</td>\n",
       "      <td>btcusdt_12h</td>\n",
       "      <td>binance_api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6153</th>\n",
       "      <td>2026-01-18 12:00:00</td>\n",
       "      <td>95168.06</td>\n",
       "      <td>95376.24</td>\n",
       "      <td>94910.00</td>\n",
       "      <td>95183.55</td>\n",
       "      <td>1256.21349</td>\n",
       "      <td>2026-01-18 23:59:59</td>\n",
       "      <td>1.194810e+08</td>\n",
       "      <td>279502</td>\n",
       "      <td>566.66885</td>\n",
       "      <td>5.389214e+07</td>\n",
       "      <td>btcusdt_12h</td>\n",
       "      <td>binance_api</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Open_time      Open      High       Low     Close      Volume  \\\n",
       "6149 2026-01-16 12:00:00  95429.09  95809.08  94293.46  95550.94  6051.25953   \n",
       "6150 2026-01-17 00:00:00  95550.94  95578.20  95021.67  95323.86  1694.35238   \n",
       "6151 2026-01-17 12:00:00  95323.85  95639.45  95050.00  95147.77  2586.44899   \n",
       "6152 2026-01-18 00:00:00  95147.77  95295.39  94876.33  95168.06  2386.33318   \n",
       "6153 2026-01-18 12:00:00  95168.06  95376.24  94910.00  95183.55  1256.21349   \n",
       "\n",
       "              Close_time  Quote_asset_volume  Number_of_trades  \\\n",
       "6149 2026-01-16 23:59:59        5.753072e+08           1962043   \n",
       "6150 2026-01-17 11:59:59        1.614328e+08            459123   \n",
       "6151 2026-01-17 23:59:59        2.467292e+08            561404   \n",
       "6152 2026-01-18 11:59:59        2.269771e+08            513805   \n",
       "6153 2026-01-18 23:59:59        1.194810e+08            279502   \n",
       "\n",
       "      Taker_buy_base_asset_volume  Taker_buy_quote_asset_volume        ts_id  \\\n",
       "6149                   2867.23124                  2.726688e+08  btcusdt_12h   \n",
       "6150                    839.78958                  8.001233e+07  btcusdt_12h   \n",
       "6151                   1220.06325                  1.163867e+08  btcusdt_12h   \n",
       "6152                   1112.37410                  1.058134e+08  btcusdt_12h   \n",
       "6153                    566.66885                  5.389214e+07  btcusdt_12h   \n",
       "\n",
       "           Source  \n",
       "6149  binance_api  \n",
       "6150  binance_api  \n",
       "6151  binance_api  \n",
       "6152  binance_api  \n",
       "6153  binance_api  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c588f19-f01c-425b-8f38-7e80b321c986",
   "metadata": {},
   "source": [
    "# Optimized version, but a single class without splitting into separate logical modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c321f30-80fa-46ef-af5d-59873231f77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from scipy.signal import find_peaks  # not used in optimized version\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator\n",
    "from numba import njit\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Numba function for period selection based on autocorrelation values\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "@njit\n",
    "def select_period(acf_values, min_period, max_period_loc, period_threshold): \n",
    "    n = acf_values.shape[0]\n",
    "    best_period = 0\n",
    "    best_acf = 0.0\n",
    "    found_strong = False\n",
    "    # Iterate through indices from min_period to max_period_loc (inclusive)\n",
    "    for i in range(min_period, min(max_period_loc+1, n)):\n",
    "        # Simple peak check: if value greater than previous and not less than next\n",
    "        if i > 0 and i < n - 1:\n",
    "            if acf_values[i] > acf_values[i - 1] and acf_values[i] >= acf_values[i + 1]:\n",
    "                if acf_values[i] >= period_threshold and not found_strong:\n",
    "                    best_period = i\n",
    "                    found_strong = True\n",
    "                elif not found_strong:\n",
    "                    if acf_values[i] > best_acf:\n",
    "                        best_acf = acf_values[i]\n",
    "                        best_period = i\n",
    "    return best_period\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# PeriodicityEstimator class with optimized period selection logic\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class PeriodicityEstimator(BaseEstimator): \n",
    "    def __init__(self, max_nlags_ratio=0.5, peak_prominence_threshold=0.1,\n",
    "                 period_threshold=0.8, min_period=2, max_period=None):\n",
    "        self.max_nlags_ratio = max_nlags_ratio\n",
    "        self.peak_prominence_threshold = peak_prominence_threshold  # not used in optimized version\n",
    "        self.period_threshold = period_threshold\n",
    "        self.min_period = min_period\n",
    "        self.max_period = max_period\n",
    "        self.period_ = None\n",
    "\n",
    "    def fit(self, X, y=None): \n",
    "        return self\n",
    "\n",
    "    def predict(self, X): \n",
    "        period = self._find_period(X)\n",
    "        self.period_ = period\n",
    "        return period\n",
    "\n",
    "    def _find_period(self, data): \n",
    "        n = len(data)\n",
    "        max_nlags = int(self.max_nlags_ratio * n)\n",
    "        max_period_loc = n // 2 if self.max_period is None else self.max_period\n",
    "        acf_values = acf(data, nlags=max_nlags, fft=True)\n",
    "        # Period selection using Numba function\n",
    "        period = select_period(acf_values, self.min_period, max_period_loc, self.period_threshold)\n",
    "        return period\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# PeriodicityDetector class with additional downsampling in estimation\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class PeriodicityDetector(BaseEstimator): \n",
    "    def __init__(self, min_period=2, max_period=None, cv=5, downsample_factor=10):\n",
    "        self.min_period = min_period\n",
    "        self.max_period = max_period\n",
    "        self.cv = cv\n",
    "        self.downsample_factor = downsample_factor\n",
    "        self.best_estimator_ = None\n",
    "\n",
    "    def fit(self, X, y=None): \n",
    "        param_grid = {\n",
    "            'max_nlags_ratio': [0.1, 0.25, 0.5],\n",
    "            'peak_prominence_threshold': [0.05, 0.1, 0.2],\n",
    "            'period_threshold': [0.5, 0.6, 0.7, 0.8],\n",
    "            'min_period': [self.min_period],\n",
    "            'max_period': [self.max_period]\n",
    "        }\n",
    "        estimator = PeriodicityEstimator()\n",
    "        grid_search = GridSearchCV(estimator, param_grid, scoring=self._score_period, cv=self.cv, n_jobs=-1)\n",
    "        # Pass data as one-dimensional array\n",
    "        grid_search.fit(X.to_numpy().ravel().reshape(-1, 1), y=[0] * len(X))\n",
    "        self.best_estimator_ = grid_search.best_estimator_\n",
    "        return self\n",
    "\n",
    "    def _score_period(self, y, period_, X=None, **kwargs): \n",
    "        period = period_[0]\n",
    "        if period == 0:\n",
    "            return -np.inf\n",
    "        try:\n",
    "            # Downsampling to speed up STL\n",
    "            X_ds = X[::self.downsample_factor]\n",
    "            from statsmodels.tsa.seasonal import STL\n",
    "            stl = STL(X_ds, period=period)\n",
    "            res = stl.fit()\n",
    "            return -res.resid.var()\n",
    "        except Exception:\n",
    "            return -np.inf\n",
    "\n",
    "    def predict(self, X): \n",
    "        return self.best_estimator_.predict(X)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Usage example (uncomment for testing)\n",
    "# -----------------------------------------------------------------------------\n",
    "# index = pd.date_range('1/1/2000', periods=365 * 10, freq='D')\n",
    "# data = pd.Series(np.sin(2 * np.pi * index.dayofyear / 12) + np.random.randn(len(index)) / 5, index=index)\n",
    "# detector = PeriodicityDetector(cv=20, downsample_factor=10)\n",
    "# detector.fit(data)\n",
    "# period = detector.predict(data)\n",
    "# print(f\"Optimal period: {period}\")\n",
    "# print(f\"Best parameters: {detector.best_estimator_.get_params()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "431bb844-8880-4b56-8f2a-b9e57b04c5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from scipy.stats import zscore, probplot\n",
    "from statsmodels.tsa.stattools import adfuller, kpss, acf\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "import statsmodels.api as sm\n",
    "import holidays\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from joblib import Parallel, delayed\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "from numba import njit\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Numba-accelerated functions for rolling trend and diagonal averaging (Hankel averaging)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "@njit\n",
    "def rolling_slope(arr, window): \n",
    "    n = arr.shape[0]\n",
    "    result = np.empty(n)\n",
    "    result[:] = np.nan\n",
    "    mean_i = (window - 1) / 2.0\n",
    "    denom = 0.0\n",
    "    for j in range(window):\n",
    "        denom += (j - mean_i) ** 2\n",
    "    for i in range(window - 1, n):\n",
    "        s = 0.0\n",
    "        for j in range(window):\n",
    "            s += (j - mean_i) * arr[i - window + 1 + j]\n",
    "        result[i] = s / denom\n",
    "    return result\n",
    "\n",
    "@njit\n",
    "def hankel_diagonal_averaging(X): \n",
    "    window_length, K = X.shape\n",
    "    N = window_length + K - 1\n",
    "    reconstructed = np.zeros(N)\n",
    "    counts = np.zeros(N)\n",
    "    for i in range(window_length):\n",
    "        for j in range(K):\n",
    "            reconstructed[i + j] += X[i, j]\n",
    "            counts[i + j] += 1.0\n",
    "    for i in range(N):\n",
    "        reconstructed[i] /= counts[i]\n",
    "    return reconstructed\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# TimeSeriesPreprocessorManager class with optimizations and logging\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class TimeSeriesPreprocessorManager:\n",
    "    \"\"\"\n",
    "    Class for time series preprocessing.\n",
    "\n",
    "    Main purpose:\n",
    "      - Prepare data for machine learning models and neural networks.\n",
    "      - Perform cleaning, missing value imputation, outlier removal, scaling,\n",
    "        decomposition, lag feature generation, and additional feature engineering.\n",
    "\n",
    "    Usage example:\n",
    "        preprocessor = TimeSeriesPreprocessorManager(\n",
    "            interval='1d',\n",
    "            timestamp_column='Open_time',\n",
    "            target_column='Open',\n",
    "            scaler='robust',\n",
    "            lag_features=5\n",
    "        )\n",
    "        X, y = preprocessor.process(raw_df)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, interval, timestamp_column='Open_time', target_column='Open', additional_columns=None, scaler='robust', lag_features=5):\n",
    "        self.interval = interval\n",
    "        self.timestamp_column = timestamp_column\n",
    "        self.target_column = target_column\n",
    "        self.additional_columns = additional_columns if additional_columns else []\n",
    "        self.scaler_type = scaler\n",
    "        self.lag_features = lag_features\n",
    "        self.original_data = None\n",
    "\n",
    "    def analyze_series(self, data): \n",
    "        analysis = {}\n",
    "        analysis['missing_values'] = data.isNone().sum()\n",
    "        z_scores = zscore(data.dropna())\n",
    "        analysis['zscore_outliers'] = int((np.abs(z_scores) > 3).sum())\n",
    "        q1 = data.quantile(0.25)\n",
    "        q3 = data.quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        analysis['iqr_outliers'] = int(((data < (q1 - 1.5 * iqr)) | (data > (q3 + 1.5 * iqr))).sum())\n",
    "        median_val = data.median()\n",
    "        mad = np.median(np.abs(data - median_val))\n",
    "        modified_z = 0.6745 * (data - median_val) / mad if mad != 0 else np.zeros_like(data)\n",
    "        analysis['mad_outliers'] = int((np.abs(modified_z) > 3.5).sum())\n",
    "        adf_result = adfuller(data.dropna())\n",
    "        analysis['adf_pvalue'] = adf_result[1]\n",
    "        stationary_adf = adf_result[1] < 0.05\n",
    "        try:\n",
    "            kpss_result = kpss(data.dropna(), regression='c', nlags=\"auto\")\n",
    "            analysis['kpss_pvalue'] = kpss_result[1]\n",
    "            stationary_kpss = kpss_result[1] > 0.05\n",
    "        except Exception as e:\n",
    "            analysis['kpss_pvalue'] = np.nan\n",
    "            stationary_kpss = False\n",
    "        window = max(30, len(data) // 10)\n",
    "        rolling_mean = data.rolling(window=window, min_periods=1).mean()\n",
    "        rolling_std = data.rolling(window=window, min_periods=1).std()\n",
    "        mean_cv = rolling_mean.std() / rolling_mean.mean() if rolling_mean.mean() != 0 else np.inf\n",
    "        std_cv = rolling_std.std() / rolling_std.mean() if rolling_std.mean() != 0 else np.inf\n",
    "        analysis['rolling_mean_cv'] = mean_cv\n",
    "        analysis['rolling_std_cv'] = std_cv\n",
    "        autocorr_lag1 = data.autocorr(lag=1)\n",
    "        analysis['lag1_autocorrelation'] = autocorr_lag1\n",
    "        qq = probplot(data.dropna(), dist=\"norm\", plot=None)\n",
    "        analysis['qq_correlation'] = qq[1][2]\n",
    "        rolling_threshold = 0.2\n",
    "        self._is_stationary = (stationary_adf and stationary_kpss and (mean_cv < rolling_threshold) and (std_cv < rolling_threshold))\n",
    "        analysis['stationary'] = self._is_stationary\n",
    "        return analysis\n",
    "\n",
    "    def detrend_series(self, data, method='linear'): \n",
    "        if method == 'linear':\n",
    "            x = np.arange(len(data))\n",
    "            coeffs = np.polyfit(x, data.values, 1)\n",
    "            trend = np.polyval(coeffs, x)\n",
    "            detrended = data - trend\n",
    "            return detrended, pd.Series(trend, index=data.index)\n",
    "        elif method == 'difference':\n",
    "            detrended = data.diff().dropna()\n",
    "            return detrended, data.iloc[0]\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported detrending method. Choose 'linear' or 'difference'.\")\n",
    "\n",
    "    def inverse_detrend(self, detrended_data, trend_component, method='linear'): \n",
    "        if method == 'linear':\n",
    "            return detrended_data + trend_component\n",
    "        elif method == 'difference':\n",
    "            return detrended_data.cumsum() + trend_component\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported inverse detrending method. Choose 'linear' or 'difference'.\")\n",
    "\n",
    "    def fill_missing_values(self, data, method='linear'):\n",
    "        return data.interpolate(method=method).bfill().ffill()\n",
    "\n",
    "    def _optimize_contamination(self, data, method='isolation_forest'): \n",
    "        if not isinstance(data, pd.Series):\n",
    "            data = data[self.target_column]\n",
    "        iqr = data.quantile(0.75) - data.quantile(0.25)\n",
    "        lower_bound = data.quantile(0.25) - 1.5 * iqr\n",
    "        upper_bound = data.quantile(0.75) + 1.5 * iqr\n",
    "        estimated_outliers = ((data < lower_bound) | (data > upper_bound)).sum()\n",
    "        estimated_contamination = max(estimated_outliers / len(data), 0.001)\n",
    "        contamination_range = np.geomspace(max(0.0001, estimated_contamination / 2), \n",
    "                                            min(0.51, estimated_contamination * 2), 10)\n",
    "        best_contamination = 0.001\n",
    "        best_score = float('inf')\n",
    "    \n",
    "        def evaluate_contamination(contamination): \n",
    "            try:\n",
    "                if method == 'isolation_forest':\n",
    "                    processed_data = self.remove_outliers_with_isolation_forest(data.copy(), contamination)\n",
    "                elif method == 'lof':\n",
    "                    processed_data = self.remove_outliers_with_lof(data.copy(), contamination)\n",
    "                else:\n",
    "                    raise ValueError(\"Unsupported method for contamination optimization.\")\n",
    "                residual_diff = data - processed_data\n",
    "                score = np.mean(np.abs(residual_diff))\n",
    "                autocorr_score = residual_diff.autocorr()\n",
    "                final_score = score * (1 - abs(autocorr_score))\n",
    "                return contamination, final_score\n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating contamination {contamination}: {e}\")\n",
    "                return contamination, float('inf')\n",
    "    \n",
    "        results = Parallel(n_jobs=-1)(delayed(evaluate_contamination)(cont) for cont in contamination_range)\n",
    "        for contamination, score in results:\n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_contamination = contamination\n",
    "        return best_contamination\n",
    "\n",
    "    def remove_outliers_with_isolation_forest(self, data, contamination=None):\n",
    "        if contamination is None:\n",
    "            contamination = self._optimize_contamination(data, method='isolation_forest')\n",
    "        reshaped_data = data.values.reshape(-1, 1)\n",
    "        isol_forest = IsolationForest(contamination=contamination)\n",
    "        outliers = isol_forest.fit_predict(reshaped_data)\n",
    "        median_value = np.median(data[~np.isin(outliers, -1)])\n",
    "        data[outliers == -1] = median_value\n",
    "        return data\n",
    "\n",
    "    def remove_outliers_with_lof(self, data, contamination=None, n_neighbors=None): \n",
    "        if contamination is None:\n",
    "            contamination = self._optimize_contamination(data, method='lof')\n",
    "        if n_neighbors is None:\n",
    "            n_neighbors = min(20, max(5, int(len(data) * 0.05)))\n",
    "        reshaped_data = data.values.reshape(-1, 1)\n",
    "        lof = LocalOutlierFactor(contamination=contamination, n_neighbors=n_neighbors)\n",
    "        outliers = lof.fit_predict(reshaped_data)\n",
    "        median_value = np.median(data[outliers == 1])\n",
    "        data[outliers == -1] = median_value\n",
    "        return data\n",
    "\n",
    "    def decompose_and_remove_outliers(self, data, method='isolation_forest', contamination=None): \n",
    "        trend, seasonal, residual = self.decompose_series(data, method='STL')\n",
    "        if method == 'isolation_forest':\n",
    "            residual_cleaned = self.remove_outliers_with_isolation_forest(residual, contamination)\n",
    "        elif method == 'lof':\n",
    "            residual_cleaned = self.remove_outliers_with_lof(residual, contamination)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported method for outlier removal.\")\n",
    "        cleaned_series = trend + seasonal + residual_cleaned\n",
    "        return cleaned_series\n",
    "\n",
    "    def remove_outliers(self, data, method='combined', contamination=None): \n",
    "        if method == 'combined':\n",
    "            remove_outliers_method = 'isolation_forest' if len(data) > 1000 else 'lof'\n",
    "            return self.decompose_and_remove_outliers(data.copy(), method=remove_outliers_method, contamination=contamination)\n",
    "        elif method == 'isolation_forest':\n",
    "            return self.remove_outliers_with_isolation_forest(data.copy(), contamination)\n",
    "        elif method == 'lof':\n",
    "            return self.remove_outliers_with_lof(data.copy(), contamination)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported outlier removal method.\")\n",
    "\n",
    "    def determine_period(self, data): \n",
    "        # Use PeriodicityDetector, ACF and FFT\n",
    "        # (Assumes PeriodicityDetector is defined somewhere in the environment)\n",
    "        detector = PeriodicityDetector(min_period=2, max_period=None, cv=10)\n",
    "        detector.fit(data)\n",
    "        period_detector = detector.predict(data)\n",
    "        period_candidates = []\n",
    "        if period_detector > 1:\n",
    "            print(f\"[LOG] Applying PeriodicityDetector: {period_detector}\")\n",
    "            period_candidates.append(period_detector)\n",
    "        acf_vals = acf(data.dropna(), nlags=min(100, len(data)//2))\n",
    "        for lag, val in enumerate(acf_vals[1:], start=1):\n",
    "            if val > 0.3:\n",
    "                print(f\"[LOG] Applying ACF: selected lag={lag}\")\n",
    "                period_candidates.append(lag)\n",
    "                break\n",
    "        fft_data = data.dropna().values\n",
    "        fft_vals = np.abs(np.fft.rfft(fft_data))\n",
    "        freqs = np.fft.rfftfreq(len(fft_data), d=1)\n",
    "        if len(fft_vals) > 1:\n",
    "            index_candidate = np.argmax(fft_vals[1:]) + 1\n",
    "            if index_candidate >= len(freqs):\n",
    "                index_candidate = len(freqs) - 1\n",
    "            dominant_freq = freqs[index_candidate]\n",
    "            default_max_period = {\n",
    "                '1h': 168,    # 24*7 - week for hourly data\n",
    "                '12h': 60,    # approximately 30 days\n",
    "                '1d': 182,    # approximately half year for daily data\n",
    "                '3d': 60,     # approximately 180 days/3 ≈ 60 observations for 3-day interval\n",
    "                '1w': 52,     # 52 weeks (year) for weekly data\n",
    "                '1M': 12      # 12 months for monthly data\n",
    "            }\n",
    "            max_allowed_period = default_max_period.get(self.interval, len(data)/2)\n",
    "            if dominant_freq > 0:\n",
    "                period_fft = int(round(1 / dominant_freq))\n",
    "                if period_fft < max_allowed_period:\n",
    "                    #print(f\"[LOG] Applying FFT: period={period_fft}\")\n",
    "                    period_candidates.append(period_fft)\n",
    "                else:\n",
    "                    print(f\"[LOG] Ignoring FFT candidate: {period_fft} (exceeds threshold)\")\n",
    "        if period_candidates:\n",
    "            period = int(np.round(np.median(period_candidates)))\n",
    "        else:\n",
    "            period = 0\n",
    "        if period < 2:\n",
    "            print(\"[LOG] Applying Fallback for period\")\n",
    "            default_periods = {'1h': 24, '12h': 14, '1d': 7, '3d': 7, '1w': 52, '1M': 12}\n",
    "            period = default_periods.get(self.interval, 1)\n",
    "        print(f\"[LOG] Determined period: {period}, candidates: {period_candidates}\")\n",
    "        return period, period_candidates\n",
    "\n",
    "    def get_seasonality_config(self, interval, data_length): \n",
    "        configs = {\n",
    "            '1h': {'daily': True, 'weekly': False, 'yearly': False},\n",
    "            '12h': {'daily': True, 'weekly': True, 'yearly': False},\n",
    "            '1d': {'daily': False, 'weekly': True, 'yearly': True},\n",
    "            '3d': {'daily': False, 'weekly': True, 'yearly': True},\n",
    "            '1w': {'daily': False, 'weekly': False, 'yearly': True},\n",
    "            '1M': {'daily': False, 'weekly': False, 'yearly': True}\n",
    "        }\n",
    "        config = configs.get(interval, {'daily': False, 'weekly': False, 'yearly': False})\n",
    "        if data_length < 365:\n",
    "            config['yearly'] = False\n",
    "        return config\n",
    "\n",
    "    def decompose_series(self, data, method='auto', freq=None, model='auto'): \n",
    "        if not isinstance(data.index, pd.DatetimeIndex):\n",
    "            raise ValueError(\"Data must have a DatetimeIndex for seasonal decomposition.\")\n",
    "        adf_pvalue = adfuller(data.dropna())[1]\n",
    "        model_type = 'additive' if adf_pvalue < 0.05 else 'multiplicative'\n",
    "        period, period_candidates = self.determine_period(data)\n",
    "        method = self._determine_method_auto(data, period, period_candidates) if method == 'auto' else method\n",
    "        decomposition_methods = {\n",
    "            'STL': self._decompose_stl,\n",
    "            'TBATS': self._decompose_tbats,\n",
    "            'SSA': self._decompose_ssa,\n",
    "            'fourier': self._decompose_fourier,\n",
    "            'prophet': self._decompose_prophet\n",
    "        }\n",
    "        if method in decomposition_methods:\n",
    "            return decomposition_methods[method](data, period, period_candidates, model_type)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported decomposition method: {method}\")\n",
    "\n",
    "    def _determine_method_auto(self, data, period, candidates): \n",
    "        if period < 2:\n",
    "            return 'SSA'\n",
    "        methods = ['STL', 'SSA', 'fourier']\n",
    "        quality = {}\n",
    "        for method in methods:\n",
    "            result = getattr(self, f'_decompose_{method.lower()}')(data, period, candidates)\n",
    "            quality[method] = np.nanmean(np.abs(result[2].dropna()))\n",
    "        return min(quality, key=quality.get)\n",
    "\n",
    "    def _decompose_stl(self, data, period, candidates, model_type=None): \n",
    "        print('[LOG] Decomposition: STL method selected')\n",
    "        stl = STL(data, period=period, robust=True)\n",
    "        result = stl.fit()\n",
    "        return result.trend, result.seasonal, result.resid\n",
    "\n",
    "    def _decompose_tbats(self, data, period, candidates, model_type=None): \n",
    "        print('[LOG] Decomposition: TBATS method selected')\n",
    "        seasonal_periods = candidates if len(candidates) > 1 else [period]\n",
    "        if self.interval == '1h':\n",
    "            seasonal_periods = [24, 168]\n",
    "        print(f\"[LOG] TBATS: seasonal periods {seasonal_periods}\")\n",
    "        estimator = TBATS(seasonal_periods=seasonal_periods)\n",
    "        tbats_model = estimator.fit(data)\n",
    "        fitted = tbats_model.y_hat\n",
    "        try:\n",
    "            seasonal_array = np.sum(tbats_model.seasonal_components_, axis=1)\n",
    "        except AttributeError:\n",
    "            seasonal_array = np.zeros_like(fitted)\n",
    "        trend_array = fitted - seasonal_array\n",
    "        resid_array = data.values.flatten() - fitted\n",
    "        trend = pd.Series(trend_array, index=data.index)\n",
    "        seasonal = pd.Series(seasonal_array, index=data.index)\n",
    "        resid = pd.Series(resid_array, index=data.index)\n",
    "        return trend, seasonal, resid\n",
    "\n",
    "    def _decompose_ssa(self, data, period, candidates, model_type=None): \n",
    "        print('[LOG] Decomposition: SSA method selected')\n",
    "        return self.ssa_decompose(data, window_length=period, variance_threshold=0.9)\n",
    "\n",
    "    def _decompose_fourier(self, data, period, candidates, model_type=None): \n",
    "        print('[LOG] Decomposition: FOURIER method selected')\n",
    "        # Add timing log to verify harmonic loop\n",
    "        t_start = time.perf_counter()\n",
    "        result = self.fourier_decompose(data, period=period)\n",
    "        t_end = time.perf_counter()\n",
    "        print(f\"[LOG] FOURIER-decompose time: {(t_end - t_start)*1000:.2f} ms\")\n",
    "        return result\n",
    "\n",
    "    def _decompose_prophet(self, data, period, candidates, model_type=None): \n",
    "        print('[LOG] Decomposition: Prophet method selected')\n",
    "        df = pd.DataFrame(data)\n",
    "        df.reset_index(inplace=True)\n",
    "        df.columns = ['ds', 'y']\n",
    "        data_length = len(df)\n",
    "        seasonality_config = self.get_seasonality_config(self.interval, data_length)\n",
    "        components = [comp for comp, flag in seasonality_config.items() if flag]\n",
    "        model_prophet = Prophet(\n",
    "            daily_seasonality=seasonality_config.get('daily', False),\n",
    "            weekly_seasonality=seasonality_config.get('weekly', False),\n",
    "            yearly_seasonality=seasonality_config.get('yearly', False),\n",
    "            changepoint_prior_scale=0.05,\n",
    "            seasonality_prior_scale=10,\n",
    "            seasonality_mode='multiplicative'\n",
    "        )\n",
    "        model_prophet.fit(df)\n",
    "        forecast = model_prophet.predict(df)\n",
    "        trend_series = pd.Series(forecast['trend'].values, index=data.index)\n",
    "        seasonal_series = pd.Series(forecast[components].sum(axis=1).values, index=data.index)\n",
    "        resid_series = pd.Series((df['y'] - forecast['trend'] - forecast[components].sum(axis=1)).values, index=data.index)\n",
    "        return trend_series, seasonal_series, resid_series\n",
    "\n",
    "    def fourier_decompose(self, data, period): \n",
    "        # Pre-compute sine and cosine features for max_k\n",
    "        data_series = pd.Series(data.values.flatten(), index=data.index)\n",
    "        df = pd.DataFrame({'y': data_series.values}, index=data_series.index)\n",
    "        t = np.arange(len(df))\n",
    "        max_k = 5\n",
    "        const = np.ones_like(t)\n",
    "        harmonics = np.arange(1, max_k+1)\n",
    "        sin_full = np.sin(2 * np.pi * np.outer(t, harmonics) / period)\n",
    "        cos_full = np.cos(2 * np.pi * np.outer(t, harmonics) / period)\n",
    "        best_aic = np.inf\n",
    "        best_model = None\n",
    "        best_X = None\n",
    "        for k in range(1, max_k+1):\n",
    "            X = np.column_stack((const, t, sin_full[:, :k], cos_full[:, :k]))\n",
    "            X = sm.add_constant(X, has_constant='add')\n",
    "            model_fit = sm.OLS(df['y'], X).fit()\n",
    "            if model_fit.aic < best_aic:\n",
    "                best_aic = model_fit.aic\n",
    "                best_model = model_fit\n",
    "                best_X = X.copy()\n",
    "        params = best_model.params\n",
    "        trend_values = best_X[:, 1] * params[1] + best_X[:, 0]\n",
    "        seasonal_values = best_X[:, 2:] @ params[2:]\n",
    "        return (pd.Series(trend_values, index=df.index),\n",
    "                pd.Series(seasonal_values, index=df.index),\n",
    "                data_series - pd.Series(trend_values, index=df.index) - pd.Series(seasonal_values, index=df.index))\n",
    "\n",
    "    def ssa_decompose(self, data, window_length=None, variance_threshold=0.9): \n",
    "        series = data.values.flatten() if isinstance(data, pd.Series) else np.array(data).flatten()\n",
    "        N = series.shape[0]\n",
    "        if window_length is None:\n",
    "            period, _ = self.determine_period(data)\n",
    "            window_length = period if period > 1 else N // 2\n",
    "        K = N - window_length + 1\n",
    "        X = as_strided(series, shape=(window_length, K), strides=(series.strides[0], series.strides[0])).copy()\n",
    "        U, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "        total_variance = np.sum(s**2)\n",
    "        explained_variance = np.cumsum(s**2) / total_variance\n",
    "        r = np.searchsorted(explained_variance, variance_threshold) + 1\n",
    "        X_reconstructed = np.zeros_like(X)\n",
    "        for i in range(r):\n",
    "            X_reconstructed += s[i] * np.outer(U[:, i], Vt[i, :])\n",
    "        reconstructed = hankel_diagonal_averaging(X_reconstructed)\n",
    "        if r >= 2:\n",
    "            X_trend = s[0] * np.outer(U[:, 0], Vt[0, :]) + s[1] * np.outer(U[:, 1], Vt[1, :])\n",
    "            trend_reconstructed = hankel_diagonal_averaging(X_trend)\n",
    "            seasonal_reconstructed = reconstructed - trend_reconstructed\n",
    "        else:\n",
    "            trend_reconstructed = reconstructed\n",
    "            seasonal_reconstructed = np.zeros_like(reconstructed)\n",
    "        residual = series - reconstructed\n",
    "        if hasattr(data, 'index'):\n",
    "            trend_reconstructed = pd.Series(trend_reconstructed, index=data.index)\n",
    "            seasonal_reconstructed = pd.Series(seasonal_reconstructed, index=data.index)\n",
    "            residual = pd.Series(residual, index=data.index)\n",
    "        return trend_reconstructed, seasonal_reconstructed, residual\n",
    "\n",
    "    def scale_data(self, data): \n",
    "        initial_stats = {\n",
    "            'min': data.min(),\n",
    "            'max': data.max(),\n",
    "            'mean': data.mean(),\n",
    "            'std': data.std(),\n",
    "            'median': data.median(),\n",
    "            'IQR': data.quantile(0.75) - data.quantile(0.25)\n",
    "        }\n",
    "        #print(\"[LOG] Initial data stats:\", initial_stats)\n",
    "        reshaped_data = data.values.reshape(-1, 1)\n",
    "        if self.scaler_type == 'minmax':\n",
    "            scaler = MinMaxScaler()\n",
    "        elif self.scaler_type == 'standard':\n",
    "            scaler = StandardScaler()\n",
    "        elif self.scaler_type == 'robust':\n",
    "            scaler = RobustScaler()\n",
    "        elif self.scaler_type == 'quantile':\n",
    "            scaler = QuantileTransformer(output_distribution='normal')\n",
    "        elif self.scaler_type == 'log':\n",
    "            scaled_data = np.log1p(reshaped_data)\n",
    "            self.custom_inverse = lambda x: np.expm1(x)\n",
    "            scaled_data = scaled_data.flatten()\n",
    "            log_stats = {\n",
    "                'min': scaled_data.min(),\n",
    "                'max': scaled_data.max(),\n",
    "                'mean': scaled_data.mean(),\n",
    "                'std': scaled_data.std()\n",
    "            }\n",
    "            print(\"[LOG] Data after log scaling stats:\", log_stats)\n",
    "            return scaled_data\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported scaler type. Choose from: 'minmax', 'standard', 'robust', 'quantile', 'log'.\")\n",
    "    \n",
    "        scaled_data = scaler.fit_transform(reshaped_data).flatten()\n",
    "        after_stats = {\n",
    "            'min': scaled_data.min(),\n",
    "            'max': scaled_data.max(),\n",
    "            'mean': scaled_data.mean(),\n",
    "            'std': scaled_data.std()\n",
    "        }\n",
    "        print(\"[LOG] Scaled data stats:\", after_stats)\n",
    "        self.scaler = scaler\n",
    "        return scaled_data\n",
    "\n",
    "    def inverse_scale(self, scaled_data): \n",
    "        if self.scaler_type == 'log' and hasattr(self, 'custom_inverse'):\n",
    "            reshaped_data = scaled_data.reshape(-1, 1)\n",
    "            inversed = self.custom_inverse(reshaped_data)\n",
    "            return inversed.flatten()\n",
    "        else:\n",
    "            reshaped_data = scaled_data.reshape(-1, 1)\n",
    "            return self.scaler.inverse_transform(reshaped_data).flatten()\n",
    "\n",
    "    def add_features(self, df, windows=None, holiday_country='US'): \n",
    "        t0 = time.perf_counter()\n",
    "        df = df.copy()\n",
    "        if not np.issubdtype(df.index.dtype, np.datetime64):\n",
    "            df.index = pd.to_datetime(df.index, errors='coerce')\n",
    "        base_col = f'{self.target_column}_scaled' if f'{self.target_column}_scaled' in df.columns else self.target_column\n",
    "        features = df.copy()\n",
    "        features[base_col] = df[base_col]\n",
    "        # Vectorized lag feature creation\n",
    "        lag_config = {\n",
    "            '1h': [1, 12, 24, 168],\n",
    "            '12h': [1, 2, 3, 14],\n",
    "            '1d': [1, 7, 14, 30],\n",
    "            '3d': [1, 2, 7],\n",
    "            '1w': [1, 2, 4],\n",
    "            '1M': [1, 3, 6, 12]\n",
    "        }\n",
    "        lags = lag_config.get(self.interval, list(range(1, self.lag_features + 1)))\n",
    "        lag_features = pd.concat([df[base_col].shift(lag).rename(f'lag_{lag}') for lag in lags], axis=1)\n",
    "        features = pd.concat([features, lag_features], axis=1)\n",
    "        default_windows = {\n",
    "            '1h': [3, 6, 12, 24],\n",
    "            '12h': [2, 4, 6, 14],\n",
    "            '1d': [3, 7, 14, 30],\n",
    "            '3d': [2, 4, 7],\n",
    "            '1w': [2, 4, 6],\n",
    "            '1M': [3, 6, 12]\n",
    "        }\n",
    "        if windows is None:\n",
    "            windows = default_windows.get(self.interval, [3, 5, 7])\n",
    "        for window in windows:\n",
    "            if window > len(df):\n",
    "                continue\n",
    "            features[f'roll_mean_{window}'] = df[base_col].rolling(window=window, min_periods=1).mean()\n",
    "            features[f'roll_std_{window}'] = df[base_col].rolling(window=window, min_periods=1).std()\n",
    "            features[f'roll_min_{window}'] = df[base_col].rolling(window=window, min_periods=1).min()\n",
    "            features[f'roll_max_{window}'] = df[base_col].rolling(window=window, min_periods=1).max()\n",
    "            features[f'roll_median_{window}'] = df[base_col].rolling(window=window, min_periods=1).median()\n",
    "            if window > 2:\n",
    "                features[f'roll_skew_{window}'] = df[base_col].rolling(window=window, min_periods=1).skew()\n",
    "            if window > 3:\n",
    "                features[f'roll_kurtosis_{window}'] = df[base_col].rolling(window=window, min_periods=1).kurt() \n",
    "        features['first_derivative'] = df[self.target_column].diff()\n",
    "        features['second_derivative'] = df[self.target_column].diff(2)\n",
    "        for window in windows:\n",
    "            features[f'rolling_trend_{window}'] = rolling_slope(df[self.target_column].values, window)\n",
    "        features['returns'] = df[self.target_column].pct_change()\n",
    "        vol_window = windows[len(windows)//2] if windows else 7\n",
    "        features['volatility'] = features['returns'].rolling(window=vol_window, min_periods=1).std()\n",
    "        \n",
    "        try:\n",
    "            years = pd.to_datetime(df.index).year.unique().tolist()\n",
    "            country_holidays = holidays.CountryHoliday(holiday_country, years=years, expand=True, observed=True)\n",
    "            features['is_holiday'] = pd.Series(df.index.map(lambda d: d.date() in country_holidays), index=df.index)\n",
    "        except Exception:\n",
    "            features['is_holiday'] = False    \n",
    "        features.to_csv('features.csv', index=False)\n",
    "        features = features.dropna()\n",
    "        t1 = time.perf_counter()\n",
    "        return features\n",
    "\n",
    "    def process(self, df): \n",
    "        try:\n",
    "            t_start = time.perf_counter()\n",
    "            df = df.copy()\n",
    "            if self.timestamp_column not in df.columns:\n",
    "                raise ValueError(f\"Timestamp column '{self.timestamp_column}' missing.\")\n",
    "            if not np.issubdtype(df[self.timestamp_column].dtype, np.datetime64):\n",
    "                df[self.timestamp_column] = pd.to_datetime(df[self.timestamp_column], errors='coerce')\n",
    "            self.original_data = df.copy()\n",
    "            df.set_index(self.timestamp_column, inplace=True)\n",
    "            for col in [self.target_column] + self.additional_columns:\n",
    "                df[col] = self.fill_missing_values(df[col])\n",
    "            _ = self.analyze_series(df[self.target_column])  # Analysis for diagnostics (no output)\n",
    "            df[f'{self.target_column}_removed_outliers'] = self.remove_outliers(df[self.target_column])\n",
    "            scaled_series = self.scale_data(df[f'{self.target_column}_removed_outliers'])\n",
    "            df[f'{self.target_column}_scaled'] = scaled_series\n",
    "            df = self.add_features(df)\n",
    "            df.sort_index(inplace=True)\n",
    "            t_end = time.perf_counter()\n",
    "            print(f\"Pipeline finished. Observations: {len(df)}, Total time: {(t_end-t_start):.2f} s\")\n",
    "            y = df[f'{self.target_column}_scaled']\n",
    "            X = df.drop(columns=[f'{self.target_column}_scaled'])            \n",
    "            return X, y\n",
    "        except Exception as e:\n",
    "            raise Exception(f'[ml_manager/TimeSeriesPreprocessorManager] error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9c8aa21-8dc6-4f4b-8470-819ec1b80021",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gb/tqzzyhfj48zf1dmpwjhzh_g40000gn/T/ipykernel_67597/1013231189.py:99: InterpolationWarning: The test statistic is outside of the range of p-values available in the\n",
      "look-up table. The actual p-value is smaller than the p-value returned.\n",
      "\n",
      "  kpss_result = kpss(data.dropna(), regression='c', nlags=\"auto\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] Applying ACF: selected lag=1\n",
      "[LOG] Ignoring FFT candidate: 3077 (exceeds threshold)\n",
      "[LOG] Applying Fallback for period\n",
      "[LOG] Determined period: 14, candidates: [1]\n",
      "[LOG] Decomposition: STL method selected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing plotly failed. Interactive plots will not work.\n",
      "Importing plotly failed. Interactive plots will not work.\n",
      "Importing plotly failed. Interactive plots will not work.\n",
      "Importing plotly failed. Interactive plots will not work.\n",
      "Importing plotly failed. Interactive plots will not work.\n",
      "Importing plotly failed. Interactive plots will not work.\n",
      "Importing plotly failed. Interactive plots will not work.\n",
      "Importing plotly failed. Interactive plots will not work.\n",
      "Importing plotly failed. Interactive plots will not work.\n",
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] Scaled data stats: {'min': -0.4956435849448443, 'max': 1.9788124482795066, 'mean': 0.2052625923849962, 'std': 0.6769338149376591}\n",
      "Pipeline finished. Observations: 6140, Total time: 4.76 s\n"
     ]
    }
   ],
   "source": [
    "# Create instance of TimeSeriesPreprocessorManager class\n",
    "ts_preprocessor_manager = TimeSeriesPreprocessorManager('12h')\n",
    "X, y = ts_preprocessor_manager.process(ts_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c702dbdc-7ca2-4e47-9888-f29d778bb9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gb/tqzzyhfj48zf1dmpwjhzh_g40000gn/T/ipykernel_67597/1013231189.py:99: InterpolationWarning: The test statistic is outside of the range of p-values available in the\n",
      "look-up table. The actual p-value is smaller than the p-value returned.\n",
      "\n",
      "  kpss_result = kpss(data.dropna(), regression='c', nlags=\"auto\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] Applying ACF: selected lag=1\n",
      "[LOG] Ignoring FFT candidate: 3077 (exceeds threshold)\n",
      "[LOG] Applying Fallback for period\n",
      "[LOG] Determined period: 14, candidates: [1]\n",
      "[LOG] Decomposition: STL method selected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing plotly failed. Interactive plots will not work.\n",
      "Importing plotly failed. Interactive plots will not work.\n",
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] Scaled data stats: {'min': -0.4974152791818321, 'max': 1.9865186856425734, 'mean': 0.20491798290368937, 'std': 0.6786758593977295}\n",
      "Pipeline finished. Observations: 6140, Total time: 2.40 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gb/tqzzyhfj48zf1dmpwjhzh_g40000gn/T/ipykernel_67597/1013231189.py:99: InterpolationWarning: The test statistic is outside of the range of p-values available in the\n",
      "look-up table. The actual p-value is smaller than the p-value returned.\n",
      "\n",
      "  kpss_result = kpss(data.dropna(), regression='c', nlags=\"auto\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] Applying ACF: selected lag=1\n",
      "[LOG] Ignoring FFT candidate: 3077 (exceeds threshold)\n",
      "[LOG] Applying Fallback for period\n",
      "[LOG] Determined period: 14, candidates: [1]\n",
      "[LOG] Decomposition: STL method selected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] Scaled data stats: {'min': -0.49376756423326656, 'max': 1.971322604266468, 'mean': 0.20579590107544674, 'std': 0.675346222284365}\n",
      "Pipeline finished. Observations: 6140, Total time: 2.32 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gb/tqzzyhfj48zf1dmpwjhzh_g40000gn/T/ipykernel_67597/1013231189.py:99: InterpolationWarning: The test statistic is outside of the range of p-values available in the\n",
      "look-up table. The actual p-value is smaller than the p-value returned.\n",
      "\n",
      "  kpss_result = kpss(data.dropna(), regression='c', nlags=\"auto\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] Applying ACF: selected lag=1\n",
      "[LOG] Ignoring FFT candidate: 3077 (exceeds threshold)\n",
      "[LOG] Applying Fallback for period\n",
      "[LOG] Determined period: 14, candidates: [1]\n",
      "[LOG] Decomposition: STL method selected\n",
      "[LOG] Scaled data stats: {'min': -0.4966945454183908, 'max': 1.9830083134755325, 'mean': 0.20550805487265383, 'std': 0.6782184282221602}\n",
      "Pipeline finished. Observations: 6140, Total time: 3.66 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gb/tqzzyhfj48zf1dmpwjhzh_g40000gn/T/ipykernel_67597/1013231189.py:99: InterpolationWarning: The test statistic is outside of the range of p-values available in the\n",
      "look-up table. The actual p-value is smaller than the p-value returned.\n",
      "\n",
      "  kpss_result = kpss(data.dropna(), regression='c', nlags=\"auto\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] Applying ACF: selected lag=1\n",
      "[LOG] Ignoring FFT candidate: 3077 (exceeds threshold)\n",
      "[LOG] Applying Fallback for period\n",
      "[LOG] Determined period: 14, candidates: [1]\n",
      "[LOG] Decomposition: STL method selected\n",
      "[LOG] Scaled data stats: {'min': -0.4954972490579902, 'max': 1.9782282154086845, 'mean': 0.2057853787761662, 'std': 0.677269054794273}\n",
      "Pipeline finished. Observations: 6140, Total time: 1.41 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gb/tqzzyhfj48zf1dmpwjhzh_g40000gn/T/ipykernel_67597/1013231189.py:99: InterpolationWarning: The test statistic is outside of the range of p-values available in the\n",
      "look-up table. The actual p-value is smaller than the p-value returned.\n",
      "\n",
      "  kpss_result = kpss(data.dropna(), regression='c', nlags=\"auto\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] Applying ACF: selected lag=1\n",
      "[LOG] Ignoring FFT candidate: 3077 (exceeds threshold)\n",
      "[LOG] Applying Fallback for period\n",
      "[LOG] Determined period: 14, candidates: [1]\n",
      "[LOG] Decomposition: STL method selected\n",
      "[LOG] Scaled data stats: {'min': -0.49652220624731935, 'max': 1.982320264830496, 'mean': 0.20559954092568533, 'std': 0.6781223080166273}\n",
      "Pipeline finished. Observations: 6140, Total time: 1.43 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gb/tqzzyhfj48zf1dmpwjhzh_g40000gn/T/ipykernel_67597/1013231189.py:99: InterpolationWarning: The test statistic is outside of the range of p-values available in the\n",
      "look-up table. The actual p-value is smaller than the p-value returned.\n",
      "\n",
      "  kpss_result = kpss(data.dropna(), regression='c', nlags=\"auto\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] Applying ACF: selected lag=1\n",
      "[LOG] Ignoring FFT candidate: 3077 (exceeds threshold)\n",
      "[LOG] Applying Fallback for period\n",
      "[LOG] Determined period: 14, candidates: [1]\n",
      "[LOG] Decomposition: STL method selected\n",
      "[LOG] Scaled data stats: {'min': -0.49703005500691144, 'max': 1.9849802226044058, 'mean': 0.2052480813500624, 'std': 0.6783630927780284}\n",
      "Pipeline finished. Observations: 6140, Total time: 1.70 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gb/tqzzyhfj48zf1dmpwjhzh_g40000gn/T/ipykernel_67597/1013231189.py:99: InterpolationWarning: The test statistic is outside of the range of p-values available in the\n",
      "look-up table. The actual p-value is smaller than the p-value returned.\n",
      "\n",
      "  kpss_result = kpss(data.dropna(), regression='c', nlags=\"auto\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] Applying ACF: selected lag=1\n",
      "[LOG] Ignoring FFT candidate: 3077 (exceeds threshold)\n",
      "[LOG] Applying Fallback for period\n",
      "[LOG] Determined period: 14, candidates: [1]\n",
      "[LOG] Decomposition: STL method selected\n",
      "[LOG] Scaled data stats: {'min': -0.49564392414993436, 'max': 1.9788138025253301, 'mean': 0.2053218522011902, 'std': 0.677025304707114}\n",
      "Pipeline finished. Observations: 6140, Total time: 1.85 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gb/tqzzyhfj48zf1dmpwjhzh_g40000gn/T/ipykernel_67597/1013231189.py:99: InterpolationWarning: The test statistic is outside of the range of p-values available in the\n",
      "look-up table. The actual p-value is smaller than the p-value returned.\n",
      "\n",
      "  kpss_result = kpss(data.dropna(), regression='c', nlags=\"auto\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] Applying ACF: selected lag=1\n",
      "[LOG] Ignoring FFT candidate: 3077 (exceeds threshold)\n",
      "[LOG] Applying Fallback for period\n",
      "[LOG] Determined period: 14, candidates: [1]\n",
      "[LOG] Decomposition: STL method selected\n",
      "[LOG] Scaled data stats: {'min': -0.49689234817383865, 'max': 1.9844302652378172, 'mean': 0.20481401254059484, 'std': 0.6780442935186475}\n",
      "Pipeline finished. Observations: 6140, Total time: 2.15 s\n",
      "2.08 s ± 719 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Error evaluating contamination 0.51: The 'contamination' parameter of IsolationForest must be a str among {'auto'} or a float in the range (0.0, 0.5]. Got 0.51 instead.\n",
      "Error evaluating contamination 0.51: The 'contamination' parameter of IsolationForest must be a str among {'auto'} or a float in the range (0.0, 0.5]. Got 0.51 instead.\n",
      "Error evaluating contamination 0.51: The 'contamination' parameter of IsolationForest must be a str among {'auto'} or a float in the range (0.0, 0.5]. Got 0.51 instead.\n",
      "Error evaluating contamination 0.51: The 'contamination' parameter of IsolationForest must be a str among {'auto'} or a float in the range (0.0, 0.5]. Got 0.51 instead.\n",
      "Error evaluating contamination 0.51: The 'contamination' parameter of IsolationForest must be a str among {'auto'} or a float in the range (0.0, 0.5]. Got 0.51 instead.\n",
      "Error evaluating contamination 0.51: The 'contamination' parameter of IsolationForest must be a str among {'auto'} or a float in the range (0.0, 0.5]. Got 0.51 instead.\n",
      "Error evaluating contamination 0.51: The 'contamination' parameter of IsolationForest must be a str among {'auto'} or a float in the range (0.0, 0.5]. Got 0.51 instead.\n",
      "Error evaluating contamination 0.51: The 'contamination' parameter of IsolationForest must be a str among {'auto'} or a float in the range (0.0, 0.5]. Got 0.51 instead.\n",
      "Error evaluating contamination 0.51: The 'contamination' parameter of IsolationForest must be a str among {'auto'} or a float in the range (0.0, 0.5]. Got 0.51 instead.\n"
     ]
    }
   ],
   "source": [
    "%timeit ts_preprocessor_manager.process(ts_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
